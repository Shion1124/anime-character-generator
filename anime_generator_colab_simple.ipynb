{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4556ab5",
   "metadata": {},
   "source": [
    "# ğŸ¨ anime-character-generator\n",
    "## Stable Diffusion + PyTorch (Colabç‰ˆãƒ»ã‚·ãƒ³ãƒ—ãƒ«)\n",
    "\n",
    "Google Colab ç’°å¢ƒã§å‹•ä½œç¢ºèªæ¸ˆã¿ã®ç°¡æ˜“ç‰ˆã§ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: GPU ç¢ºèª\n",
    "import torch\n",
    "print(f\"âœ“ GPU Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæœ€æ–°ç‰ˆãƒ»ç«¶åˆå›é¿ï¼‰\n",
    "!pip install -q --upgrade setuptools wheel\n",
    "!pip install -q diffusers transformers accelerate safetensors\n",
    "!pip install -q pillow torchvision\n",
    "print(\"âœ… Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e1feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"âœ… All libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32\n",
    "\n",
    "print(f\"ğŸ“¦ Loading model... (device: {device})\")\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    torch_dtype=dtype,\n",
    "    safety_checker=None\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "pipe.enable_attention_slicing()\n",
    "\n",
    "print(\"âœ… Model ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: æ„Ÿæƒ…ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ\n",
    "import os\n",
    "os.makedirs(\"outputs/emotions\", exist_ok=True)\n",
    "\n",
    "emotions = {\n",
    "    \"happy\": \"happy smile, cheerful, joyful\",\n",
    "    \"angry\": \"angry expression, intense eyes\",\n",
    "    \"sad\": \"sad expression, melancholic\",\n",
    "    \"surprised\": \"surprised expression, wide eyes\"\n",
    "}\n",
    "\n",
    "base = \"1girl, anime character, masterpiece, high quality\"\n",
    "emotion_images = {}\n",
    "\n",
    "print(\"\\nğŸ­ GENERATING EMOTIONS...\\n\")\n",
    "\n",
    "for emotion_name, emotion_desc in emotions.items():\n",
    "    prompt = f\"{base}, {emotion_desc}\"\n",
    "    print(f\"  [{emotion_name.upper()}] Generating...\", end=\"\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"low quality, blurry\",\n",
    "            num_inference_steps=20,\n",
    "            guidance_scale=7.0,\n",
    "            height=512,\n",
    "            width=512\n",
    "        ).images[0]\n",
    "    \n",
    "    filepath = f\"outputs/emotions/character_{emotion_name}.png\"\n",
    "    image.save(filepath)\n",
    "    emotion_images[emotion_name] = image\n",
    "    print(\" âœ…\")\n",
    "\n",
    "print(\"\\nâœ… Emotions complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49089231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ\n",
    "os.makedirs(\"outputs/styles\", exist_ok=True)\n",
    "\n",
    "styles = {\n",
    "    \"with_hat\": \"wearing hat\",\n",
    "    \"with_earrings\": \"wearing earrings\",\n",
    "    \"formal\": \"formal dress, elegant\",\n",
    "    \"casual\": \"casual outfit\",\n",
    "    \"with_makeup\": \"with makeup, beautiful\",\n",
    "    \"glasses\": \"wearing glasses\"\n",
    "}\n",
    "\n",
    "style_images = {}\n",
    "\n",
    "print(\"ğŸ‘— GENERATING STYLES...\\n\")\n",
    "\n",
    "for style_name, style_desc in styles.items():\n",
    "    prompt = f\"{base}, {style_desc}\"\n",
    "    print(f\"  [{style_name.upper()}] Generating...\", end=\"\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = pipe(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=\"low quality, blurry\",\n",
    "            num_inference_steps=20,\n",
    "            guidance_scale=7.0,\n",
    "            height=512,\n",
    "            width=512\n",
    "        ).images[0]\n",
    "    \n",
    "    filepath = f\"outputs/styles/character_{style_name}.png\"\n",
    "    image.save(filepath)\n",
    "    style_images[style_name] = image\n",
    "    print(\" âœ…\")\n",
    "\n",
    "print(\"\\nâœ… Styles complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b67c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: çµæœè¡¨ç¤ºï¼ˆæ„Ÿæƒ…ï¼‰\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, img) in enumerate(emotion_images.items()):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(name.upper(), fontsize=12, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"emotion_results.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Emotion results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a9eefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: çµæœè¡¨ç¤ºï¼ˆã‚¹ã‚¿ã‚¤ãƒ«ï¼‰\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, img) in enumerate(style_images.items()):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(name.upper(), fontsize=11, fontweight='bold')\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"style_results.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Style results saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860e1cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æº–å‚™\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nğŸ“ Generated {len(emotion_images) + len(style_images)} images\")\n",
    "print(\"\\nğŸ“¥ To download:\")\n",
    "print(\"   1. Left sidebar â†’ Files icon\")\n",
    "print(\"   2. outputs/ folder â†’ Right click\")\n",
    "print(\"   3. Download\")\n",
    "print(\"\\nâœ¨ Success!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
