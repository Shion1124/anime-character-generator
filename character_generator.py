#!/usr/bin/env python3
"""
anime-character-generator
Stable Diffusion + PyTorch ã‚’æ´»ç”¨ã—ãŸã€ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªå‹•ç”Ÿæˆ

Usage:
    python character_generator.py --emotion happy --style casual
    python character_generator.py --all
"""

import argparse
import os
import torch
from pathlib import Path
from diffusers import StableDiffusionPipeline
from PIL import Image, ImageDraw, ImageFont
import re


class AnimeCharacterGenerator:
    def __init__(self, device: str = "auto"):
        """
        åˆæœŸåŒ–
        
        Args:
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ ('cuda', 'cpu', or 'auto')
        """
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        
        print(f"ğŸ“¦ Device: {self.device} | Dtype: {self.dtype}")
        print(f"âœ“ GPU Available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("\nğŸ“¦ Loading Stable Diffusion v1.5...")
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=self.dtype,
            safety_checker=None
        )
        self.pipe = self.pipe.to(self.device)
        self.pipe.enable_attention_slicing()
        print("âœ… Model ready!")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.base_prompt = "1girl, anime character, masterpiece, high quality"
        
        # æ„Ÿæƒ…å®šç¾©
        self.emotions = {
            "happy": "happy smile, cheerful, joyful",
            "angry": "angry expression, intense eyes",
            "sad": "sad expression, melancholic",
            "surprised": "surprised expression, wide eyes"
        }
        
        # ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
        self.styles = {
            "with_hat": "wearing hat, stylish, fashionable",
            "with_earrings": "wearing earrings, jewelry, elegant",
            "with_makeup": "with makeup, beautiful, glamorous",
            "formal": "wearing formal dress, elegant, professional",
            "casual": "casual outfit, relaxed, friendly",
            "long_hair": "long brown hair, soft flowing hair",
            "blush": "soft blush on cheeks",
            "fireplace": "warm fireplace in background",
            "warm_lighting": "warm ambient lighting, soft orange glow",
            "cozy_room": "cozy indoor setting",
            "bokeh": "cinematic bokeh lights",
            "portrait": "upper body portrait",
            "depth_of_field": "shallow depth of field",
            "high_detail": "highly detailed",
            "soft_shading": "soft anime shading",
            "masterpiece": "masterpiece, best quality"
        }
    
    def generate_image(
        self,
        prompt: str,
        output_path: str = None,
        num_inference_steps: int = 20,
        guidance_scale: float = 7.0,
        height: int = 512,
        width: int = 512,
        seed: int = None
    ) -> Image.Image:
        """
        å˜ä¸€ç”»åƒç”Ÿæˆ
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            output_path: ä¿å­˜å…ˆãƒ‘ã‚¹
            num_inference_steps: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«
            height: ç”»åƒé«˜ã•
            width: ç”»åƒå¹…
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            
        Returns:
            PIL Image
        """
        if seed is not None:
            torch.manual_seed(seed)
        
        with torch.no_grad():
            image = self.pipe(
                prompt=prompt,
                negative_prompt="low quality, blurry",
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                height=height,
                width=width
            ).images[0]
        
        if output_path:
            os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
            image.save(output_path)
            print(f"  âœ… Saved: {output_path}")
        
        return image
    
    def generate_emotions(self, output_dir: str = "./outputs/emotions") -> dict:
        """
        å…¨æ„Ÿæƒ…ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        
        Returns:
            {emotion_name: PIL.Image} ã®è¾æ›¸
        """
        print("\nğŸ­ GENERATING EMOTIONS...\n")
        os.makedirs(output_dir, exist_ok=True)
        
        results = {}
        for emotion_name, emotion_desc in self.emotions.items():
            prompt = f"{self.base_prompt}, {emotion_desc}"
            print(f"  [{emotion_name.upper()}] Generating...", end="", flush=True)
            
            filepath = os.path.join(output_dir, f"character_{emotion_name}.png")
            image = self.generate_image(prompt, filepath)
            results[emotion_name] = image
        
        print(f"\nâœ… Emotions generation complete!")
        return results
    
    def generate_styles(self, output_dir: str = "./outputs/styles") -> dict:
        """
        å…¨ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        
        Returns:
            {style_name: PIL.Image} ã®è¾æ›¸
        """
        print("\nğŸ‘— GENERATING STYLES...\n")
        os.makedirs(output_dir, exist_ok=True)
        
        results = {}
        for style_name, style_desc in self.styles.items():
            prompt = f"{self.base_prompt}, {style_desc}"
            print(f"  [{style_name.upper()}] Generating...", end="", flush=True)
            
            filepath = os.path.join(output_dir, f"character_{style_name}.png")
            image = self.generate_image(prompt, filepath)
            results[style_name] = image
        
        print(f"\nâœ… Styles generation complete!")
        return results
    
    def generate_all(self):
        """å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ + çµæœè¡¨ç¤º"""
        emotion_images = self.generate_emotions()
        style_images = self.generate_styles()
        
        # ã‚°ãƒªãƒƒãƒ‰å½¢å¼ã§åˆæˆ
        print("\nğŸ“Š Creating composite grid images...")
        self._create_grid_composite(emotion_images, "emotion_results", rows=2, cols=2)
        self._create_grid_composite(style_images, "style_results", rows=2, cols=4)
        
        print("\n" + "="*60)
        print("âœ… GENERATION COMPLETE!")
        print("="*60)
        print(f"\nğŸ“ Generated {len(emotion_images) + len(style_images)} images")
        print(f"ğŸ“ Output directory: ./outputs/")
    
    def _get_next_version(self, base_filename: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å–å¾—
        ä¾‹: style_results_v1.png â†’ style_results_v2.png
        """
        output_dir = "./outputs"
        existing_files = []
        
        if os.path.exists(output_dir):
            existing_files = os.listdir(output_dir)
        
        # ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«åã«åˆè‡´ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        pattern = rf'^{re.escape(base_filename)}_v(\d+)\.png$'
        versions = []
        
        for fn in existing_files:
            match = re.match(pattern, fn)
            if match:
                versions.append(int(match.group(1)))
        
        # æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯æœ€å¤§å€¤+1ã€ãªã„å ´åˆã¯1
        next_version = max(versions) + 1 if versions else 1
        return f"{base_filename}_v{next_version}.png"
    
    def _create_grid_composite(
        self, 
        images: dict, 
        base_filename: str,
        rows: int = 2, 
        cols: int = 2,
        img_size: int = 512,
        gap: int = 10
    ):
        """
        è¤‡æ•°ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§åˆæˆ
        
        Args:
            images: {name: PIL.Image} ã®è¾æ›¸
            base_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ãªã—ï¼‰
            rows: ã‚°ãƒªãƒƒãƒ‰è¡Œæ•°
            cols: ã‚°ãƒªãƒƒãƒ‰åˆ—æ•°
            img_size: å„ç”»åƒã®ã‚µã‚¤ã‚º
            gap: ç”»åƒé–“ã®ã‚®ãƒ£ãƒƒãƒ—
        """
        os.makedirs("./outputs", exist_ok=True)
        
        # ä½¿ç”¨ã™ã‚‹ç”»åƒã‚’å–å¾—ï¼ˆæœ€å¤§ rows*colsï¼‰
        use_images = list(images.items())[:rows*cols]
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºè¨ˆç®—
        canvas_width = cols * img_size + (cols - 1) * gap + gap * 2
        canvas_height = rows * img_size + (rows - 1) * gap + gap * 2
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ä½œæˆï¼ˆç™½èƒŒæ™¯ï¼‰
        canvas = Image.new('RGB', (canvas_width, canvas_height), color='white')
        
        # å„ç”»åƒã‚’ãƒšãƒ¼ã‚¹ãƒˆ
        for idx, (name, img) in enumerate(use_images):
            row = idx // cols
            col = idx % cols
            
            # ãƒšãƒ¼ã‚¹ãƒˆä½ç½®
            x = gap + col * (img_size + gap)
            y = gap + row * (img_size + gap)
            
            # ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ãƒšãƒ¼ã‚¹ãƒˆ
            resized_img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)
            canvas.paste(resized_img, (x, y))
        
        # æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å–å¾—ã—ã¦ä¿å­˜
        output_filename = self._get_next_version(base_filename)
        output_path = f"./outputs/{output_filename}"
        
        canvas.save(output_path, quality=95)
        print(f"  âœ… Saved: {output_filename}")
    
    def _display_results(self, images: dict, output_file: str, rows: int, cols: int):
        """çµæœç”»åƒã‚’è¡¨ç¤ºãƒ»ä¿å­˜ï¼ˆéæ¨å¥¨ï¼šäº’æ›æ€§ã§æ®‹ã™ï¼‰"""
        # ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ _create_grid_composite ã«ç½®ãæ›ã‚ã£ãŸ
        pass


def main():
    parser = argparse.ArgumentParser(description="ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªå‹•ç”Ÿæˆ")
    parser.add_argument("--emotion", choices=["happy", "angry", "sad", "surprised"],
                       help="æ„Ÿæƒ…ã‚’æŒ‡å®š")
    parser.add_argument("--style", 
                       choices=["with_hat", "with_earrings", "with_makeup", "formal", "casual",
                               "long_hair", "blush", "fireplace", "warm_lighting", "cozy_room",
                               "bokeh", "portrait", "depth_of_field", "high_detail", 
                               "soft_shading", "masterpiece"],
                       help="ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒ‡å®š")
    parser.add_argument("--all", action="store_true", help="å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ")
    parser.add_argument("--device", choices=["cuda", "cpu"], default="auto",
                       help="å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹")
    
    args = parser.parse_args()
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–
    generator = AnimeCharacterGenerator(device=args.device)
    
    if args.all:
        generator.generate_all()
    elif args.emotion and args.style:
        # ç‰¹å®šã®æ„Ÿæƒ…+ã‚¹ã‚¿ã‚¤ãƒ«ã§ç”Ÿæˆ
        emotion_desc = generator.emotions[args.emotion]
        style_desc = generator.styles[args.style]
        prompt = f"{generator.base_prompt}, {emotion_desc}, {style_desc}"
        print(f"\nğŸ¨ Generating: {args.emotion} + {args.style}")
        image = generator.generate_image(prompt)
        image.show()
    elif args.emotion:
        # æ„Ÿæƒ…ã®ã¿ã§ç”Ÿæˆ
        generator.generate_emotions()
    elif args.style:
        # ã‚¹ã‚¿ã‚¤ãƒ«ã®ã¿ã§ç”Ÿæˆ
        generator.generate_styles()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
