#!/usr/bin/env python3
"""
anime-character-generator
Stable Diffusion + PyTorch ã‚’æ´»ç”¨ã—ãŸã€ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªå‹•ç”Ÿæˆ

Usage:
    python character_generator.py --emotion happy --style casual
    python character_generator.py --all
"""

import argparse
import os
import torch
from pathlib import Path
from diffusers import StableDiffusionPipeline
from PIL import Image
import matplotlib.pyplot as plt


class AnimeCharacterGenerator:
    def __init__(self, device: str = "auto"):
        """
        åˆæœŸåŒ–
        
        Args:
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ ('cuda', 'cpu', or 'auto')
        """
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
            
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        
        print(f"ğŸ“¦ Device: {self.device} | Dtype: {self.dtype}")
        print(f"âœ“ GPU Available: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print("\nğŸ“¦ Loading Stable Diffusion v1.5...")
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=self.dtype,
            safety_checker=None
        )
        self.pipe = self.pipe.to(self.device)
        self.pipe.enable_attention_slicing()
        print("âœ… Model ready!")
        
        # ãƒ™ãƒ¼ã‚¹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        self.base_prompt = "1girl, anime character, masterpiece, high quality"
        
        # æ„Ÿæƒ…å®šç¾©
        self.emotions = {
            "happy": "happy smile, cheerful, joyful",
            "angry": "angry expression, intense eyes",
            "sad": "sad expression, melancholic",
            "surprised": "surprised expression, wide eyes"
        }
        
        # ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
        self.styles = {
            "with_hat": "wearing hat",
            "with_earrings": "wearing earrings",
            "formal": "formal dress, elegant",
            "casual": "casual outfit",
            "with_makeup": "with makeup, beautiful",
            "glasses": "wearing glasses"
        }
    
    def generate_image(
        self,
        prompt: str,
        output_path: str = None,
        num_inference_steps: int = 20,
        guidance_scale: float = 7.0,
        height: int = 512,
        width: int = 512,
        seed: int = None
    ) -> Image.Image:
        """
        å˜ä¸€ç”»åƒç”Ÿæˆ
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            output_path: ä¿å­˜å…ˆãƒ‘ã‚¹
            num_inference_steps: æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚¹ã‚±ãƒ¼ãƒ«
            height: ç”»åƒé«˜ã•
            width: ç”»åƒå¹…
            seed: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
            
        Returns:
            PIL Image
        """
        if seed is not None:
            torch.manual_seed(seed)
        
        with torch.no_grad():
            image = self.pipe(
                prompt=prompt,
                negative_prompt="low quality, blurry",
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                height=height,
                width=width
            ).images[0]
        
        if output_path:
            os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
            image.save(output_path)
            print(f"  âœ… Saved: {output_path}")
        
        return image
    
    def generate_emotions(self, output_dir: str = "./outputs/emotions") -> dict:
        """
        å…¨æ„Ÿæƒ…ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        
        Returns:
            {emotion_name: PIL.Image} ã®è¾æ›¸
        """
        print("\nğŸ­ GENERATING EMOTIONS...\n")
        os.makedirs(output_dir, exist_ok=True)
        
        results = {}
        for emotion_name, emotion_desc in self.emotions.items():
            prompt = f"{self.base_prompt}, {emotion_desc}"
            print(f"  [{emotion_name.upper()}] Generating...", end="", flush=True)
            
            filepath = os.path.join(output_dir, f"character_{emotion_name}.png")
            image = self.generate_image(prompt, filepath)
            results[emotion_name] = image
        
        print(f"\nâœ… Emotions generation complete!")
        return results
    
    def generate_styles(self, output_dir: str = "./outputs/styles") -> dict:
        """
        å…¨ã‚¹ã‚¿ã‚¤ãƒ«ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        
        Returns:
            {style_name: PIL.Image} ã®è¾æ›¸
        """
        print("\nğŸ‘— GENERATING STYLES...\n")
        os.makedirs(output_dir, exist_ok=True)
        
        results = {}
        for style_name, style_desc in self.styles.items():
            prompt = f"{self.base_prompt}, {style_desc}"
            print(f"  [{style_name.upper()}] Generating...", end="", flush=True)
            
            filepath = os.path.join(output_dir, f"character_{style_name}.png")
            image = self.generate_image(prompt, filepath)
            results[style_name] = image
        
        print(f"\nâœ… Styles generation complete!")
        return results
    
    def generate_all(self):
        """å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ + çµæœè¡¨ç¤º"""
        emotion_images = self.generate_emotions()
        style_images = self.generate_styles()
        
        # æ„Ÿæƒ…çµæœè¡¨ç¤º
        self._display_results(emotion_images, "emotion_results.png", rows=2, cols=2)
        
        # ã‚¹ã‚¿ã‚¤ãƒ«çµæœè¡¨ç¤º
        self._display_results(style_images, "style_results.png", rows=2, cols=3)
        
        print("\n" + "="*60)
        print("âœ… GENERATION COMPLETE!")
        print("="*60)
        print(f"\nğŸ“ Generated {len(emotion_images) + len(style_images)} images")
        print(f"ğŸ“ Output directory: ./outputs/")
    
    def _display_results(self, images: dict, output_file: str, rows: int, cols: int):
        """çµæœç”»åƒã‚’è¡¨ç¤ºãƒ»ä¿å­˜"""
        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*5))
        axes = axes.flatten() if rows * cols > 1 else [axes]
        
        for idx, (name, img) in enumerate(list(images.items())[:rows*cols]):
            axes[idx].imshow(img)
            axes[idx].set_title(name.upper(), fontsize=12, fontweight='bold')
            axes[idx].axis('off')
        
        # ä½™ã£ãŸè»¸ã‚’éè¡¨ç¤º
        for idx in range(len(images), rows*cols):
            fig.delaxes(axes[idx])
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=100, bbox_inches='tight')
        plt.close()
        print(f"ğŸ“Š Saved: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è‡ªå‹•ç”Ÿæˆ")
    parser.add_argument("--emotion", choices=["happy", "angry", "sad", "surprised"],
                       help="æ„Ÿæƒ…ã‚’æŒ‡å®š")
    parser.add_argument("--style", choices=["with_hat", "with_earrings", "formal", "casual", "with_makeup", "glasses"],
                       help="ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æŒ‡å®š")
    parser.add_argument("--all", action="store_true", help="å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ")
    parser.add_argument("--device", choices=["cuda", "cpu"], default="auto",
                       help="å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹")
    
    args = parser.parse_args()
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–
    generator = AnimeCharacterGenerator(device=args.device)
    
    if args.all:
        generator.generate_all()
    elif args.emotion and args.style:
        # ç‰¹å®šã®æ„Ÿæƒ…+ã‚¹ã‚¿ã‚¤ãƒ«ã§ç”Ÿæˆ
        emotion_desc = generator.emotions[args.emotion]
        style_desc = generator.styles[args.style]
        prompt = f"{generator.base_prompt}, {emotion_desc}, {style_desc}"
        print(f"\nğŸ¨ Generating: {args.emotion} + {args.style}")
        image = generator.generate_image(prompt)
        image.show()
    elif args.emotion:
        # æ„Ÿæƒ…ã®ã¿ã§ç”Ÿæˆ
        generator.generate_emotions()
    elif args.style:
        # ã‚¹ã‚¿ã‚¤ãƒ«ã®ã¿ã§ç”Ÿæˆ
        generator.generate_styles()
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
