#!/usr/bin/env python3
"""
anime-character-generator v1.0
PyTorch + Diffusers ã‚’ç”¨ã„ãŸã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”Ÿæˆãƒ„ãƒ¼ãƒ«

ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ã€‘
Version: 1.0
Date: 2026-02-17
Status: ãƒ–ãƒ­ã‚°ã§å®Œå…¨èª¬æ˜ã•ã‚Œã‚‹åŸºæœ¬å®Ÿè£…ç‰ˆ

å®Ÿè£…ä¸Šã®å·¥å¤«ï¼š
1. GPU ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æœ€é©åŒ–
2. ãƒãƒƒãƒå‡¦ç†ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–
3. ã‚¨ãƒ©ãƒ¼æ™‚ã®å®‰å…¨ãªå‡¦ç†
4. è©³ç´°ãªãƒ­ã‚°å‡ºåŠ›

ã€è«–æ–‡ãƒ™ãƒ¼ã‚¹ã€‘
- Ho et al. (2020): DDPM ã®åŸºç¤ç†è«–
  URL: https://arxiv.org/abs/2006.11239
- Rombach et al. (2022): Stable Diffusion v1.5
  URL: https://arxiv.org/abs/2112.10752
"""

from diffusers import StableDiffusionPipeline
import torch
from pathlib import Path
from datetime import datetime
import argparse
import json
import os
import re
from typing import Dict, Tuple
from PIL import Image

class AnimeCharacterGenerator:
    """ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, device: str = "auto", model_id: str = "runwayml/stable-diffusion-v1-5"):
        """
        åˆæœŸåŒ–å‡¦ç†
        
        Args:
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ ('cuda', 'cpu', or 'auto')
            model_id: Hugging Face ã®ãƒ¢ãƒ‡ãƒ« ID
        
        Raises:
            RuntimeError: GPU ãŒåˆ©ç”¨ä¸å¯ãªå ´åˆ
        """
        # ãƒ‡ãƒã‚¤ã‚¹æ±ºå®š
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        if self.device == "cuda" and not torch.cuda.is_available():
            raise RuntimeError("CUDA requested but not available")
        
        self.dtype = torch.float16 if self.device == "cuda" else torch.float32
        
        print(f"ğŸ“± Device: {self.device}")
        print(f"ğŸ“Š Precision: {self.dtype}")
        
        if self.device == "cuda":
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
            print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
        print(f"\nğŸ“¦ Loading {model_id}...")
        self.pipe = StableDiffusionPipeline.from_pretrained(
            model_id,
            torch_dtype=self.dtype,
            safety_checker=None  # æ¨è«–é«˜é€ŸåŒ–
        )
        self.pipe = self.pipe.to(self.device)
        self.pipe.enable_attention_slicing()  # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
        
        print("âœ… Model loaded successfully")
        
        # ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾©
        self.base_prompt = "1girl, anime character, masterpiece, high quality"
        self.emotions = {
            "happy": "happy smile, cheerful, joyful",
            "angry": "angry expression, intense eyes",
            "sad": "sad expression, melancholic",
            "surprised": "surprised expression, wide eyes"
        }
        self.styles = {
            "with_hat": "wearing hat, stylish, fashionable",
            "with_earrings": "wearing earrings, jewelry, elegant",
            "with_makeup": "with makeup, beautiful, glamorous",
            "formal": "wearing formal dress, elegant, professional",
            "casual": "casual outfit, relaxed, friendly",
            "long_hair": "long brown hair, soft flowing hair",
            "blush": "soft blush on cheeks",
            "fireplace": "warm fireplace in background",
            "warm_lighting": "warm ambient lighting, soft orange glow",
            "cozy_room": "cozy indoor setting",
            "bokeh": "cinematic bokeh lights",
            "portrait": "upper body portrait",
            "depth_of_field": "shallow depth of field",
            "high_detail": "highly detailed",
            "soft_shading": "soft anime shading",
            "masterpiece": "masterpiece, best quality"
        }
    
    def generate_image(
        self,
        prompt: str,
        negative_prompt: str = "low quality, blurry",
        num_steps: int = 20,
        guidance_scale: float = 7.0,
        seed: int = None
    ) -> Image.Image:
        """å˜ä¸€ç”»åƒç”Ÿæˆ"""
        
        if seed is not None:
            torch.manual_seed(seed)
            torch.cuda.manual_seed(seed)
        
        with torch.no_grad():
            image = self.pipe(
                prompt=prompt,
                negative_prompt=negative_prompt,
                num_inference_steps=num_steps,
                guidance_scale=guidance_scale,
                height=512,
                width=512
            ).images[0]
        
        return image
    
    def generate_collection(
        self,
        collection_type: str = "all",
        output_dir: str = "./outputs"
    ) -> Dict[str, str]:
        """
        è¤‡æ•°ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ
        
        Args:
            collection_type: 'emotions', 'styles', 'all'
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
        Returns:
            {name: filepath} ã®è¾æ›¸
        """
        output_path = Path(output_dir) / collection_type
        output_path.mkdir(parents=True, exist_ok=True)
        
        results = {}
        prompts_to_generate = {}
        
        if collection_type in ["emotions", "all"]:
            prompts_to_generate.update(self.emotions)
        if collection_type in ["styles", "all"]:
            prompts_to_generate.update(self.styles)
        
        total = len(prompts_to_generate)
        
        for idx, (name, desc) in enumerate(prompts_to_generate.items(), 1):
            full_prompt = f"{self.base_prompt}, {desc}"
            
            print(f"[{idx}/{total}] Generating: {name}...", end="", flush=True)
            
            # ãƒ¡ãƒ¢ãƒªæ¸…ç†
            torch.cuda.empty_cache()
            
            try:
                image = self.generate_image(full_prompt)
                filepath = output_path / f"character_{name}.png"
                image.save(str(filepath))
                results[name] = str(filepath)
                print(" âœ…")
            
            except Exception as e:
                print(f" âŒ Error: {e}")
                continue
        
        return results
    
    def _get_next_version(self, base_filename: str) -> str:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å–å¾—
        ä¾‹: style_results_v1.png â†’ style_results_v2.png
        """
        output_dir = "./outputs"
        existing_files = os.listdir(output_dir) if os.path.exists(output_dir) else []
        
        pattern = rf'^{re.escape(base_filename)}_v(\d+)\.png$'
        versions = []
        
        for fn in existing_files:
            match = re.match(pattern, fn)
            if match:
                versions.append(int(match.group(1)))
        
        next_version = max(versions) + 1 if versions else 1
        return f"{base_filename}_v{next_version}.png"
    
    def _create_grid_composite(
        self, 
        images: Dict[str, Image.Image], 
        base_filename: str,
        rows: int = 2, 
        cols: int = 2,
        img_size: int = 512,
        gap: int = 10
    ) -> Image.Image:
        """
        è¤‡æ•°ç”»åƒã‚’ã‚°ãƒªãƒƒãƒ‰ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã§åˆæˆ
        
        Args:
            images: {name: PIL.Image} ã®è¾æ›¸
            base_filename: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ãªã—ï¼‰
            rows: ã‚°ãƒªãƒƒãƒ‰è¡Œæ•°
            cols: ã‚°ãƒªãƒƒãƒ‰åˆ—æ•°
            img_size: å„ç”»åƒã®ã‚µã‚¤ã‚º
            gap: ç”»åƒé–“ã®ã‚®ãƒ£ãƒƒãƒ—
        
        Returns:
            åˆæˆæ¸ˆã¿ã® PIL Image
        """
        os.makedirs("./outputs", exist_ok=True)
        
        # ä½¿ç”¨ã™ã‚‹ç”»åƒã‚’å–å¾—ï¼ˆæœ€å¤§ rows*colsï¼‰
        use_images = list(images.items())[:rows*cols]
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºè¨ˆç®—
        canvas_width = cols * img_size + (cols - 1) * gap + gap * 2
        canvas_height = rows * img_size + (rows - 1) * gap + gap * 2
        
        # ã‚­ãƒ£ãƒ³ãƒã‚¹ä½œæˆï¼ˆç™½èƒŒæ™¯ï¼‰
        canvas = Image.new('RGB', (canvas_width, canvas_height), color='white')
        
        # å„ç”»åƒã‚’ãƒšãƒ¼ã‚¹ãƒˆ
        for idx, (name, img) in enumerate(use_images):
            row = idx // cols
            col = idx % cols
            
            # ãƒšãƒ¼ã‚¹ãƒˆä½ç½®
            x = gap + col * (img_size + gap)
            y = gap + row * (img_size + gap)
            
            # ç”»åƒã‚’ãƒªã‚µã‚¤ã‚ºã—ã¦ãƒšãƒ¼ã‚¹ãƒˆ
            resized_img = img.resize((img_size, img_size), Image.Resampling.LANCZOS)
            canvas.paste(resized_img, (x, y))
        
        # æ¬¡ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å–å¾—ã—ã¦ä¿å­˜
        output_filename = self._get_next_version(base_filename)
        output_path = f"./outputs/{output_filename}"
        
        canvas.save(output_path, quality=95)
        print(f"âœ… Saved: {output_filename}")
        
        return canvas


def main():
    parser = argparse.ArgumentParser(description="Anime character generator v1.0")
    parser.add_argument("--emotion", choices=list(AnimeCharacterGenerator().emotions.keys()),
                       help="Generate specific emotion")
    parser.add_argument("--style", choices=list(AnimeCharacterGenerator().styles.keys()),
                       help="Generate specific style")
    parser.add_argument("--all", action="store_true", help="Generate all variations")
    
    args = parser.parse_args()
    
    # ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–
    generator = AnimeCharacterGenerator()
    
    # ç”Ÿæˆå®Ÿè¡Œ
    if args.all:
        results = generator.generate_collection(collection_type="all")
    elif args.emotion or args.style:
        # ç‰¹å®šã®çµ„ã¿åˆã‚ã›
        prompt_parts = [generator.base_prompt]
        if args.emotion:
            prompt_parts.append(generator.emotions[args.emotion])
        if args.style:
            prompt_parts.append(generator.styles[args.style])
        
        prompt = ", ".join(prompt_parts)
        image = generator.generate_image(prompt)
        
        emotion_part = args.emotion or "any"
        style_part = args.style or "any"
        image.save(f"character_{emotion_part}_{style_part}.png")
        results = {f"{emotion_part}_{style_part}": "generated"}
    else:
        parser.print_help()
        return
    
    print(f"\nâœ… Generation complete! Generated {len(results)} images")


if __name__ == "__main__":
    main()
