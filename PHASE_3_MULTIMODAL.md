# Phase 3: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ“ä½œï¼ˆImage-to-Image + ControlNetï¼‰å®Ÿè£…ã‚¬ã‚¤ãƒ‰

**å¯¾è±¡ãƒ•ã‚§ãƒ¼ã‚º**: Phase 3 (ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ‹¡å¼µ)  
**æ¨å®šæœŸé–“**: 5-7æ—¥  
**åŸºç›¤æŠ€è¡“**: Image-to-Image ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ControlNet (Llama-2 Vision backbone)  
**ä¾å­˜**: Phase 2A ã® LoRA ãƒ¢ãƒ‡ãƒ« + ä»»æ„ã§ Phase 2B ã® LCM  
**æˆæœç‰©**: åŒæ–¹å‘å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ + ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ã‚¨ãƒ³ã‚¸ãƒ³

---

## ğŸ“– èƒŒæ™¯ï¼šãªãœãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‹ï¼Ÿ

### å•é¡Œ: å˜ä¸€ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒï¼‰ã®åˆ¶é™

Phase 2A ã¾ã§ã¯ **ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ ç”»åƒç”Ÿæˆ** ã®å˜ä¸€æ–¹å‘ï¼š

```
åˆ©ç”¨ã‚·ãƒ¼ãƒ³:
  âœ… ã€Œå¤©ä½¿ã®ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’æã„ã¦ã€â†’ ç”Ÿæˆ
  âŒ ã€Œã“ã®ã‚¹ã‚±ãƒƒãƒã‚’ã‚¢ãƒ‹ãƒ¡é¢¨ã«å¤‰æ›ã—ã¦ã€
  âŒ ã€Œã“ã®å†™çœŸã®ãƒãƒ¼ã‚ºã§æ–°ã—ã„ã‚­ãƒ£ãƒ©ã‚’ç”Ÿæˆã€
  âŒ ã€Œã“ã“ã ã‘ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ã¦ã€
```

### è§£æ±ºç­–: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

Phase 3 ã§å®Ÿè£…ã™ã‚‹ 3 ã¤ã®æ–°æ©Ÿèƒ½ï¼š

```
1. Image-to-Image (I2I)
   ã‚¹ã‚±ãƒƒãƒ / ä½å“è³ªç”»åƒ â†’ ã‚¢ãƒ‹ãƒ¡é¢¨å¤‰æ›
   ä¾‹: æ‰‹æãã‚¹ã‚±ãƒƒãƒ â†’ é«˜å“è³ªã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©

2. ControlNet
   ãƒãƒ¼ã‚º / æ§‹å›³ â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”Ÿæˆ
   ä¾‹: ãƒãƒ¼ã‚ºç”»åƒ â†’ ãã®ãƒãƒ¼ã‚ºã§ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ç”Ÿæˆ

3. Inpainting
   å±€æ‰€ç·¨é›†: é«ªè‰²å¤‰æ›´ã€ã‚»ãƒ¼ã‚¿ãƒ¼è‰²å¤‰æ›´ãªã©
   ä¾‹: æ—¢å­˜ç”»åƒã®é ˜åŸŸ â†’ æ–°ã—ã„è¦ç´ ã«ç½®æ›
```

---

## ğŸ¯ ç†è«–çš„åŸºç¤

### 1. Image-to-Image (I2I) ã®ã®ä»•çµ„ã¿

```
é€šå¸¸ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:
  noise_latent â†’ [100 step] â†’ image

I2I ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³:
  input_image â†’ VAE encode â†’ image_latent
  image_latent + noise â†’ [50 step] â†’ image
  
  strength ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
    strength=0.0  â†’ å…ƒç”»åƒã®ã¾ã¾
    strength=0.5  â†’ å…ƒç”»åƒã¨ãƒã‚¤ã‚ºã‚’ 50% ãšã¤ãƒŸãƒƒã‚¯ã‚¹
    strength=1.0  â†’ å®Œå…¨ãƒã‚¤ã‚ºã‹ã‚‰å†ç”Ÿæˆ
```

### 2. ControlNet ã®æ¦‚å¿µ

```
é€šå¸¸ Diffusion:
  noise â†’ [UNet] â†’ image
  æ¡ä»¶ (ãƒ†ã‚­ã‚¹ãƒˆ) ã®ã¿

ControlNet:
  conditioning_image (ãƒãƒ¼ã‚º / ã‚¨ãƒƒã‚¸)
         â†“
    [ControlNet ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼]
         â†“
  control_embedding
         â†“
  noise + control_embedding â†’ [UNet] â†’ image
  
çµæœ: ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ + æ§‹å›³æ¡ä»¶ã§ç”Ÿæˆ
```

### 3. Inpaintingï¼ˆå±€æ‰€ç·¨é›†ï¼‰

```
é€šå¸¸:
  input_image â†’ [100 step] â†’ output_image (å…¨ä½“å¤‰æ›´)

Inpainting:
  input_image + mask â†’ VAE encode (masked region ã®ã¿ãƒã‚¤ã‚ºåŒ–)
                    â†’ [50 step]
                    â†’ output_image (masked region ã®ã¿å¤‰æ›´)

ãƒã‚¹ã‚¯:
  255 (ç™½): ç·¨é›†å¯¾è±¡ã®é ˜åŸŸ
  0 (é»’): ä¿æŒã™ã‚‹é ˜åŸŸ
```

---

## ğŸ› ï¸ å®Ÿè£…ã‚¹ãƒ†ãƒƒãƒ—

### Step 1: å…ˆè¡ŒçŸ¥è­˜ãƒ»ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸

```bash
# ControlNet å¯¾å¿œã® diffusers ãŒå¿…è¦
pip install -q diffusers>=0.21.0  # >= 0.24.0 æ¨å¥¨
pip install -q controlnet-aux  # ControlNet å‰å‡¦ç†

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python -c "from diffusers import StableDiffusionControlNetPipeline; print('âœ…')"
```

### Step 2: MultimodalPipeline ã‚¯ãƒ©ã‚¹å®Ÿè£…

**ãƒ•ã‚¡ã‚¤ãƒ«**: `multimodal_pipeline.py` ã‚’æ–°è¦ä½œæˆ

```python
#!/usr/bin/env python3
"""
Multimodal Pipeline: Image-to-Image + ControlNet + Inpainting

ä½¿ç”¨ä¾‹:
    pipeline = MultimodalPipeline(
        lora_path="./lora_weights/anime-lora-final"
    )
    
    # Image-to-Image
    output = pipeline.image_to_image(
        input_image="sketch.png",
        prompt="anime girl",
        strength=0.8
    )
    
    # ControlNet (ãƒãƒ¼ã‚º)
    output = pipeline.generate_with_pose(
        pose_image="person_pose.jpg",
        prompt="anime girl"
    )
    
    # Inpainting
    output = pipeline.inpaint(
        input_image="character.png",
        mask_image="mask.png",
        prompt="blue hair"
    )
"""

import torch
import os
from pathlib import Path
from typing import Optional, Union, List
from PIL import Image
import numpy as np
from diffusers import (
    StableDiffusionPipeline,
    StableDiffusionImg2ImgPipeline,
    StableDiffusionInpaintPipeline,
    StableDiffusionControlNetPipeline,
    ControlNetModel,
    AutoencoderKL,
    DDPMScheduler
)
from controlnet_aux import OpenposeDetector, CannyEdgeDetector


class MultimodalPipeline:
    """
    ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒã€ç”»åƒâ†’ç”»åƒã€ControlNet ã«ã‚ˆã‚‹
    è¤‡æ•°ã®ç”Ÿæˆæ–¹å¼ã‚’ã‚µãƒãƒ¼ãƒˆ
    """
    
    def __init__(
        self,
        base_model: str = "runwayml/stable-diffusion-v1-5",
        lora_path: Optional[str] = None,
        device: str = "cuda",
        dtype: torch.dtype = torch.float16,
        use_lcm: bool = False
    ):
        """
        åˆæœŸåŒ–
        
        Args:
            base_model: ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
            lora_path: LoRA ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            device: ãƒ‡ãƒã‚¤ã‚¹
            dtype: ãƒ‡ãƒ¼ã‚¿å‹
            use_lcm: LCM ã‚’ä½¿ç”¨ã™ã‚‹ã‹
        """
        
        self.device = device
        self.dtype = dtype
        self.use_lcm = use_lcm
        
        print("ğŸ“¦ Loading base model")
        
        # ãƒ†ã‚­ã‚¹ãƒˆâ†’ç”»åƒ (T2I)
        self.t2i_pipe = StableDiffusionPipeline.from_pretrained(
            base_model,
            torch_dtype=dtype,
            safety_checker=None
        ).to(device)
        
        # ç”»åƒâ†’ç”»åƒ (I2I)
        self.i2i_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            base_model,
            torch_dtype=dtype,
            safety_checker=None
        ).to(device)
        
        # Inpainting
        self.inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
            base_model,
            torch_dtype=dtype,
            safety_checker=None
        ).to(device)
        
        # LoRA çµ±åˆ
        if lora_path:
            print(f"ğŸ“š Loading LoRA: {lora_path}")
            self.t2i_pipe.unet.load_adapter(lora_path)
            self.i2i_pipe.unet.load_adapter(lora_path)
            self.inpaint_pipe.unet.load_adapter(lora_path)
        
        # ControlNetï¼ˆè¤‡æ•°ã‚¿ã‚¤ãƒ—å¯¾å¿œï¼‰
        self.controlnets = {}
        self._setup_controlnets()
        
        print("âœ… Multimodal pipelines ready")
    
    def _setup_controlnets(self):
        """ControlNet ãƒ¢ãƒ‡ãƒ«ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        
        try:
            # Canny ã‚¨ãƒƒã‚¸æ¤œå‡º
            self.controlnets["canny"] = ControlNetModel.from_pretrained(
                "lllyasviel/sd-controlnet-canny",
                torch_dtype=self.dtype
            ).to(self.device)
            print("  âœ“ Canny ControlNet loaded")
        except Exception as e:
            print(f"  âš ï¸  Canny ControlNet error: {e}")
        
        try:
            # OpenPose
            self.controlnets["openpose"] = ControlNetModel.from_pretrained(
                "lllyasviel/sd-controlnet-openpose",
                torch_dtype=self.dtype
            ).to(self.device)
            print("  âœ“ OpenPose ControlNet loaded")
        except Exception as e:
            print(f"  âš ï¸  OpenPose ControlNet error: {e}")
        
        try:
            # Depth
            self.controlnets["depth"] = ControlNetModel.from_pretrained(
                "lllyasviel/sd-controlnet-depth",
                torch_dtype=self.dtype
            ).to(self.device)
            print("  âœ“ Depth ControlNet loaded")
        except Exception as e:
            print(f"  âš ï¸  Depth ControlNet error: {e}")
    
    def load_image(self, image_path: Union[str, Path]) -> Image.Image:
        """ç”»åƒã‚’èª­ã¿è¾¼ã‚“ã§ 512x512 ã«ãƒªã‚µã‚¤ã‚º"""
        
        image = Image.open(image_path).convert("RGB")
        image = image.resize((512, 512), Image.Resampling.LANCZOS)
        return image
    
    # ============ Mode 1: Image-to-Image ============
    
    def image_to_image(
        self,
        input_image: Union[str, Image.Image],
        prompt: str,
        negative_prompt: str = "",
        strength: float = 0.8,
        num_inference_steps: int = None,
        guidance_scale: float = 7.5
    ) -> Image.Image:
        """
        Image-to-Image å¤‰æ›
        
        ç”¨é€”:
          - ã‚¹ã‚±ãƒƒãƒ â†’ é«˜å“è³ªç”»åƒ
          - ä½å“è³ªç”»åƒ â†’ ã‚¢ãƒ‹ãƒ¡é¢¨
          - ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€
        
        Args:
            input_image: å…¥åŠ›ç”»åƒ
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            negative_prompt: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            strength: å¤‰æ›´åº¦ï¼ˆ0=å…ƒã®ã¾ã¾, 1=å®Œå…¨å†ç”Ÿæˆï¼‰
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: æ¡ä»¶ã‚¬ã‚¤ãƒ‰ã®å¼·ã•
        
        Returns:
            å¤‰æ›æ¸ˆã¿ç”»åƒ
        """
        
        if isinstance(input_image, str):
            input_image = self.load_image(input_image)
        
        # LCM ã®å ´åˆ
        if self.use_lcm:
            num_inference_steps = num_inference_steps or 4
        else:
            num_inference_steps = num_inference_steps or 30
        
        print(f"ğŸ¨ Image-to-Image")
        print(f"   Strength: {strength:.1f} (0=ä¿æŒ, 1=å†ç”Ÿæˆ)")
        print(f"   Steps: {num_inference_steps}")
        
        with torch.no_grad():
            output = self.i2i_pipe(
                prompt=prompt,
                image=input_image,
                strength=strength,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                negative_prompt=negative_prompt
            ).images[0]
        
        return output
    
    # ============ Mode 2: ControlNet (Pose) ============
    
    def generate_with_pose(
        self,
        pose_image: Union[str, Image.Image],
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = None,
        guidance_scale: float = 7.5
    ) -> Image.Image:
        """
        ãƒãƒ¼ã‚ºæ¡ä»¶ä»˜ãç”Ÿæˆ
        
        ç”¨é€”:
          - äººç‰©ãƒãƒ¼ã‚ºç”»åƒ â†’ ãã®ãƒãƒ¼ã‚ºã®ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©
          - ãƒãƒ¼ã‚ºãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰æ–°è¦ç”Ÿæˆ
        
        Args:
            pose_image: ãƒãƒ¼ã‚ºç”»åƒï¼ˆOpenPose ã§æ¤œå‡ºå¯èƒ½ï¼‰
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            negative_prompt: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ«
        
        Returns:
            ç”Ÿæˆç”»åƒ
        """
        
        if "openpose" not in self.controlnets:
            raise ValueError("OpenPose ControlNet not loaded")
        
        if isinstance(pose_image, str):
            pose_image = self.load_image(pose_image)
        
        # OpenPose æ¤œå‡º
        print("ğŸ•µï¸ Detecting pose with OpenPose")
        detector = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
        pose_detected = detector(pose_image)
        
        # ControlNet ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        if self.use_lcm:
            num_inference_steps = num_inference_steps or 4
        else:
            num_inference_steps = num_inference_steps or 20
        
        print(f"ğŸ­ ControlNet (Pose)")
        print(f"   Steps: {num_inference_steps}")
        
        pipe = StableDiffusionControlNetPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            controlnet=self.controlnets["openpose"],
            torch_dtype=self.dtype,
            safety_checker=None
        ).to(self.device)
        
        # LoRA çµ±åˆ
        # pipe.unet.load_adapter(lora_path)  # å¿…è¦ã«å¿œã˜ã¦
        
        with torch.no_grad():
            output = pipe(
                prompt=prompt,
                image=pose_detected,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                negative_prompt=negative_prompt
            ).images[0]
        
        return output
    
    # ============ Mode 3: ControlNet (Edge) ============
    
    def generate_with_edges(
        self,
        edge_image: Union[str, Image.Image],
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = None,
        guidance_scale: float = 7.5
    ) -> Image.Image:
        """
        ã‚¨ãƒƒã‚¸æ¡ä»¶ä»˜ãç”Ÿæˆï¼ˆæ§‹å›³æŒ‡å®šï¼‰
        
        ç”¨é€”:
          - ã‚¹ã‚±ãƒƒãƒ ï¼ˆã‚¨ãƒƒã‚¸ï¼‰ â†’ å®Œæˆç”»åƒ
          - æ§‹å›³æŒ‡å®šç”Ÿæˆ
        
        Args:
            edge_image: ã‚¨ãƒƒã‚¸ç”»åƒ
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            negative_prompt: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ«
        
        Returns:
            ç”Ÿæˆç”»åƒ
        """
        
        if "canny" not in self.controlnets:
            raise ValueError("Canny ControlNet not loaded")
        
        if isinstance(edge_image, str):
            edge_image = self.load_image(edge_image)
        
        # ã‚¨ãƒƒã‚¸æ¤œå‡º
        print("ğŸ“ Detecting edges with Canny")
        detector = CannyEdgeDetector()
        edges = detector(edge_image)
        
        if self.use_lcm:
            num_inference_steps = num_inference_steps or 4
        else:
            num_inference_steps = num_inference_steps or 20
        
        print(f"ğŸ“ ControlNet (Edges)")
        print(f"   Steps: {num_inference_steps}")
        
        pipe = StableDiffusionControlNetPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            controlnet=self.controlnets["canny"],
            torch_dtype=self.dtype,
            safety_checker=None
        ).to(self.device)
        
        with torch.no_grad():
            output = pipe(
                prompt=prompt,
                image=edges,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                negative_prompt=negative_prompt
            ).images[0]
        
        return output
    
    # ============ Mode 4: Inpainting (å±€æ‰€ç·¨é›†) ============
    
    def inpaint(
        self,
        input_image: Union[str, Image.Image],
        mask_image: Union[str, Image.Image],
        prompt: str,
        negative_prompt: str = "",
        num_inference_steps: int = None,
        guidance_scale: float = 7.5,
        strength: float = 0.8
    ) -> Image.Image:
        """
        å±€æ‰€ç·¨é›†ï¼ˆInpaintingï¼‰
        
        ç”¨é€”:
          - é«ªè‰²å¤‰æ›´
          - æœè£…å¤‰æ›´
          - èƒŒæ™¯å¤‰æ›´
          - ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆç½®æ›
        
        Args:
            input_image: å…¥åŠ›ç”»åƒ
            mask_image: ãƒã‚¹ã‚¯ç”»åƒ (ç™½=ç·¨é›†, é»’=ä¿æŒ)
            prompt: ç·¨é›†å†…å®¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            negative_prompt: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            num_inference_steps: ã‚¹ãƒ†ãƒƒãƒ—æ•°
            guidance_scale: ã‚¬ã‚¤ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ«
            strength: ç·¨é›†ã®å¼·ã•
        
        Returns:
            ç·¨é›†æ¸ˆã¿ç”»åƒ
        """
        
        if isinstance(input_image, str):
            input_image = self.load_image(input_image)
        if isinstance(mask_image, str):
            mask_image = self.load_image(mask_image)
        
        # ãƒã‚¹ã‚¯ã‚’äºŒå€¤åŒ–ï¼ˆ0 or 255ï¼‰
        mask_array = np.array(mask_image.convert("L"))
        mask_array = (mask_array > 128).astype(np.uint8) * 255
        mask_image = Image.fromarray(mask_array)
        
        if self.use_lcm:
            num_inference_steps = num_inference_steps or 8
        else:
            num_inference_steps = num_inference_steps or 30
        
        print(f"âœï¸  Inpainting (å±€æ‰€ç·¨é›†)")
        print(f"   Prompt: {prompt[:50]}...")
        print(f"   Steps: {num_inference_steps}")
        
        with torch.no_grad():
            output = self.inpaint_pipe(
                prompt=prompt,
                image=input_image,
                mask_image=mask_image,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                negative_prompt=negative_prompt,
                strength=strength
            ).images[0]
        
        return output
    
    # ============ ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ============
    
    def save_image(self, image: Image.Image, output_path: Union[str, Path]):
        """ç”»åƒä¿å­˜"""
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        image.save(output_path)
        print(f"ğŸ’¾ Saved to {output_path}")
    
    def batch_process(
        self,
        input_dir: str,
        output_dir: str,
        mode: str = "i2i",
        prompt: str = "anime character, masterpiece",
        **kwargs
    ):
        """
        ãƒãƒƒãƒå‡¦ç†
        
        Args:
            input_dir: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            mode: å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ ("i2i", "pose", "edges", "inpaint")
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            **kwargs: ãƒ¢ãƒ¼ãƒ‰å›ºæœ‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        """
        
        input_path = Path(input_dir)
        image_files = list(input_path.glob("*.png")) + list(input_path.glob("*.jpg"))
        
        print(f"ğŸ”„ Batch processing {len(image_files)} images")
        
        for i, image_file in enumerate(image_files, 1):
            print(f"\n [{i}/{len(image_files)}] Processing {image_file.name}")
            
            try:
                if mode == "i2i":
                    output = self.image_to_image(image_file, prompt, **kwargs)
                elif mode == "pose":
                    output = self.generate_with_pose(image_file, prompt, **kwargs)
                elif mode == "edges":
                    output = self.generate_with_edges(image_file, prompt, **kwargs)
                else:
                    print(f"âš ï¸  Unknown mode: {mode}")
                    continue
                
                output_file = Path(output_dir) / image_file.name
                self.save_image(output, output_file)
            
            except Exception as e:
                print(f"âŒ Error: {e}")


def main():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    
    pipeline = MultimodalPipeline(
        lora_path="./lora_weights/anime-lora-final",
        use_lcm=False
    )
    
    # 1. Image-to-Image
    sketch = "samples/sketch.png"
    output_i2i = pipeline.image_to_image(
        input_image=sketch,
        prompt="beautiful anime girl, long hair, detailed",
        strength=0.7
    )
    pipeline.save_image(output_i2i, "outputs/i2i_result.png")
    
    # 2. ControlNet (Pose)
    pose_image = "samples/pose.jpg"
    output_pose = pipeline.generate_with_pose(
        pose_image=pose_image,
        prompt="anime girl, standing pose, beautiful"
    )
    pipeline.save_image(output_pose, "outputs/pose_result.png")
    
    # 3. Inpainting
    character = "outputs/character.png"
    mask = "outputs/hair_mask.png"
    output_inpaint = pipeline.inpaint(
        input_image=character,
        mask_image=mask,
        prompt="blue hair, long hair"
    )
    pipeline.save_image(output_inpaint, "outputs/inpaint_result.png")


if __name__ == "__main__":
    main()
```

---

## ğŸ’¡ å®Ÿç”¨ä¾‹

### ä¾‹ 1: ã‚¹ã‚±ãƒƒãƒ â†’ é«˜å“è³ªã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©

```python
pipeline = MultimodalPipeline(lora_path="./lora_weights/anime-lora-final")

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰‹æ›¸ãã‚¹ã‚±ãƒƒãƒ
user_sketch = "user_input/sketch.png"

# ã‚¹ã‚±ãƒƒãƒã‚’ã‚¢ãƒ‹ãƒ¡åŒ–
output = pipeline.image_to_image(
    input_image=user_sketch,
    prompt="beautiful girl, anime style, long hair",
    strength=0.9  # ã»ã¼å†ç”Ÿæˆï¼ˆã‚¹ã‚±ãƒƒãƒå°Šé‡ï¼‰
)

output.save("results/anime_version.png")
```

### ä¾‹ 2: ãƒãƒ¼ã‚º + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ â†’ ã‚­ãƒ£ãƒ©ç”Ÿæˆ

```python
# ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸ã‚“ã ãƒãƒ¼ã‚ºç”»åƒ
pose_reference = "references/sitting_pose.jpg"

# ãã®ãƒãƒ¼ã‚ºã§ã‚¢ãƒ‹ãƒ¡ã‚­ãƒ£ãƒ©ç”Ÿæˆ
character = pipeline.generate_with_pose(
    pose_image=pose_reference,
    prompt="anime girl, pink hair, magical girl costume"
)

character.save("results/character_with_pose.png")
```

### ä¾‹ 3: å±€æ‰€ç·¨é›†ï¼ˆé«ªè‰²å¤‰æ›´ï¼‰

```python
# æ—¢å­˜ã‚­ãƒ£ãƒ©ç”»åƒ
original = "gallery/character_v1.png"

# é«ªè‰²å¤‰æ›´ç”¨ãƒã‚¹ã‚¯ä½œæˆ
mask = create_hair_mask(original)  # åˆ¥é€”é–¢æ•°

# é«ªè‰²ã‚’é’ã«å¤‰æ›´
modified = pipeline.inpaint(
    input_image=original,
    mask_image=mask,
    prompt="blue hair, anime style"
)

modified.save("results/character_blue_hair.png")
```

---

## ğŸ“Š æœŸå¾…ã•ã‚Œã‚‹æˆæœ

| æ©Ÿèƒ½ | å…¥åŠ› | å‡¦ç†æ™‚é–“ | æ´»ç”¨ |
|-----|------|--------|------|
| Image-to-Image | ã‚¹ã‚±ãƒƒãƒ | 5-10ç§’ | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚±ãƒƒãƒâ†’é«˜å“è³ªåŒ– |
| ControlNet Pose | ãƒãƒ¼ã‚ºç”»åƒ | 8-15ç§’ | ãƒãƒ¼ã‚ºæŒ‡å®šã‚­ãƒ£ãƒ©ç”Ÿæˆ |
| ControlNet Edge | ã‚¨ãƒƒã‚¸ | 5-10ç§’ | æ§‹å›³æŒ‡å®šç”Ÿæˆ |
| Inpainting | æ—¢å­˜ç”»+é ˜åŸŸ | 3-8ç§’ | å±€æ‰€ç·¨é›†ãƒ»è‰²å¤‰æ›´ |

---

## âœ… å®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

- [ ] `multimodal_pipeline.py` å®Ÿè£…å®Œäº†
- [ ] ControlNet ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç¢ºèª
- [ ] Image-to-Image ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ControlNet (Pose) ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ControlNet (Edges) ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] Inpainting ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
- [ ] ãƒ–ãƒ­ã‚°è¨˜äº‹åŸ·ç­†ã€ŒControlNet ã§è‡ªç”±åº¦ã®é«˜ã„ç”Ÿæˆã€

---

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**: Phase 3 å®Œäº†å¾Œã€[PHASE_4_DEPLOYMENT.md](PHASE_4_DEPLOYMENT.md) ã¸

