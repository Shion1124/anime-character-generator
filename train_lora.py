#!/usr/bin/env python3
"""
Anime Impressionist LoRA Training Script

ç”¨é€”:
    Stable Diffusion v1.5 ã« LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨
    Danbooru ã‹ã‚‰åé›†ã—ãŸ 300 æšã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’

å®Ÿè¡Œä¾‹:
    # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ
    python train_lora.py --data_dir training_data --output_dir lora_weights --epochs 50
    
    # Google Colab å®Ÿè¡Œ
    !python train_lora.py --data_dir /content/training_data --output_dir /content/lora_weights --epochs 100

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
    pip install -q diffusers transformers accelerate peft pillow torch tqdm safetensors
"""

import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import traceback

# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç’°å¢ƒä¾å­˜ï¼‰
try:
    from diffusers import StableDiffusionPipeline, DDPMScheduler, AutoencoderKL, UNet2DConditionModel
    from diffusers.models.attention_processor import LoRAAttnProcessor, LoRAAttnProcessor2_0
    from diffusers.loaders import AttnProcsLayers
    from transformers import CLIPTokenizer, CLIPTextModel
    from safetensors.torch import save_file
    IMPORTS_SUCCESS = True
except ImportError as e:
    print(f"âš ï¸  Some imports failed. Will attempt installation: {e}")
    IMPORTS_SUCCESS = False


class AnimeDataset(Dataset):
    """ã‚¢ãƒ‹ãƒ¡ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, image_size: int = 512):
        """åˆæœŸåŒ–
        
        Args:
            data_dir: training_data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            image_size: å‡ºåŠ›ç”»åƒã‚µã‚¤ã‚º
        """
        self.image_size = image_size
        self.image_paths = []
        self.metadata = {}
        
        data_path = Path(data_dir)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        metadata_file = data_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                self.metadata = json.load(f)
        
        # ç”»åƒãƒ‘ã‚¹ã‚’åé›†
        for style_dir in data_path.iterdir():
            if not style_dir.is_dir() or style_dir.name.startswith("."):
                continue
            
            for img_file in style_dir.glob("*.*"):
                if img_file.suffix.lower() in [".png", ".jpg", ".jpeg"]:
                    self.image_paths.append(str(img_file))
        
        print(f"ğŸ“Š Dataset loaded: {len(self.image_paths)} images")
        
        self.transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size), 
                             interpolation=transforms.InterpolationMode.LANCZOS),
            transforms.CenterCrop(self.image_size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1, 1] ç¯„å›²ã¸æ­£è¦åŒ–
        ])
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® 1 ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
        
        img_path = self.image_paths[idx]
        
        try:
            # ç”»åƒãƒ­ãƒ¼ãƒ‰
            image = Image.open(img_path).convert("RGB")
            pixel_values = self.transform(image)
            
            # ã‚¹ã‚¿ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ï¼‰
            style_name = Path(img_path).parent.name
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆã‚¹ã‚¿ã‚¤ãƒ«å + åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
            prompt = f"{style_name}, anime, masterpiece, high quality"
            
            return {
                "pixel_values": pixel_values,
                "prompt": prompt,
                "file_path": img_path
            }
        
        except Exception as e:
            print(f"âŒ Error loading {img_path}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿”ã™
            return self[0] if idx > 0 else self[(idx + 1) % len(self)]


class LoRATrainer:
    """LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(
        self,
        model_id: str = "runwayml/stable-diffusion-v1-5",
        output_dir: str = "lora_weights",
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        lora_rank: int = 8,
        lora_alpha: float = 32,
    ):
        """åˆæœŸåŒ–
        
        Args:
            model_id: Hugging Face ãƒ¢ãƒ‡ãƒ« ID
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
            lora_rank: LoRA ãƒ©ãƒ³ã‚¯
            lora_alpha: LoRA ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
        """
        self.model_id = model_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.device = device
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        
        print("="*60)
        print("ğŸš€ LoRA Trainer Initialization")
        print("="*60)
        print(f"ğŸ“¦ Model: {model_id}")
        print(f"ğŸ“ Output: {self.output_dir}")
        print(f"ğŸ’» Device: {device}")
        print(f"ğŸ¯ LoRA Config: rank={lora_rank}, alpha={lora_alpha}")
        
        self._setup_model()
    
    def _setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ»LoRA è¨­å®šã®åˆæœŸåŒ– (diffusers ãƒã‚¤ãƒ†ã‚£ãƒ– LoRA)"""
        
        try:
            print("\nğŸ“¥ Loading Stable Diffusion v1.5...")
            dtype = torch.float16 if "cuda" in self.device else torch.float32
            
            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = CLIPTokenizer.from_pretrained(self.model_id, subfolder="tokenizer")
            self.text_encoder = CLIPTextModel.from_pretrained(self.model_id, subfolder="text_encoder", torch_dtype=dtype).to(self.device)
            self.vae = AutoencoderKL.from_pretrained(self.model_id, subfolder="vae", torch_dtype=dtype).to(self.device)
            self.unet = UNet2DConditionModel.from_pretrained(self.model_id, subfolder="unet", torch_dtype=dtype).to(self.device)
            
            # VAE ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯å‡çµ
            self.vae.requires_grad_(False)
            self.text_encoder.requires_grad_(False)
            self.unet.requires_grad_(False)
            
            # diffusers ãƒã‚¤ãƒ†ã‚£ãƒ– LoRA ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’è¨­å®š
            # PEFT ã® task_type ä¾å­˜ã‚’ä¸€åˆ‡ä½¿ã‚ãªã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
            unet_attn_procs = {}
            for name in self.unet.attn_processors.keys():
                cross_attention_dim = (
                    None
                    if name.endswith("attn1.processor")
                    else self.unet.config.cross_attention_dim
                )
                if name.startswith("mid_block"):
                    hidden_size = self.unet.config.block_out_channels[-1]
                elif name.startswith("up_blocks"):
                    block_id = int(name[len("up_blocks.")])
                    hidden_size = list(reversed(self.unet.config.block_out_channels))[block_id]
                elif name.startswith("down_blocks"):
                    block_id = int(name[len("down_blocks.")])
                    hidden_size = self.unet.config.block_out_channels[block_id]
                else:
                    hidden_size = self.unet.config.block_out_channels[0]
                
                unet_attn_procs[name] = LoRAAttnProcessor(
                    hidden_size=hidden_size,
                    cross_attention_dim=cross_attention_dim,
                    rank=self.lora_rank
                ).to(dtype=dtype, device=self.device)
            
            self.unet.set_attn_processor(unet_attn_procs)
            
            # å­¦ç¿’å¯¾è±¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: LoRA ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿
            self.lora_layers = AttnProcsLayers(self.unet.attn_processors)
            
            lora_params = sum(p.numel() for p in self.lora_layers.parameters())
            print(f"âœ… LoRA configured: {lora_params:,} trainable params")
            print(f"âœ… Model loaded and LoRA configured (diffusers native)")
            
        except Exception as e:
            print(f"âŒ Error setting up model: {e}")
            traceback.print_exc()
            raise
    
    def train(
        self,
        data_dir: str,
        num_epochs: int = 50,
        batch_size: int = 1,
        learning_rate: float = 1e-4,
        num_workers: int = 0,
        save_interval: int = 5,
    ):
        """LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        
        Args:
            data_dir: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            num_workers: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            save_interval: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰
        """
        
        print("\n" + "="*60)
        print("ğŸ“ Starting LoRA Training")
        print("="*60)
        print(f"ğŸ“Š Training Epochs: {num_epochs}")
        print(f"ğŸ“¦ Batch Size: {batch_size}")
        print(f"ğŸ¯ Learning Rate: {learning_rate}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dataset = AnimeDataset(data_dir)
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers
            )
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š (LoRA ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿)
            optimizer = torch.optim.AdamW(
                self.lora_layers.parameters(),
                lr=learning_rate
            )
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            num_training_steps = len(dataloader) * num_epochs
            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, num_training_steps
            )
            
            # ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
            noise_scheduler = DDPMScheduler.from_pretrained(
                self.model_id, subfolder="scheduler"
            )
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆLoRA ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã¿å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼‰
            self.unet.train()
            self.vae.eval()
            self.text_encoder.eval()
            
            training_log = {
                "config": {
                    "model_id": self.model_id,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "lora_rank": self.lora_rank,
                    "lora_alpha": self.lora_alpha,
                },
                "history": []
            }
            
            global_step = 0
            
            for epoch in range(num_epochs):
                print(f"\n[Epoch {epoch + 1}/{num_epochs}]")
                epoch_loss = 0.0
                
                pbar = tqdm(dataloader, desc="Training", leave=False)
                
                for batch_idx, batch in enumerate(pbar):
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                    prompts = batch["prompt"]
                    dtype = torch.float16 if "cuda" in self.device else torch.float32
                    pixel_values = batch["pixel_values"].to(device=self.device, dtype=dtype)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ & VAE ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå‹¾é…ä¸è¦ï¼‰
                    with torch.no_grad():
                        input_ids = self.tokenizer(
                            prompts,
                            max_length=self.tokenizer.model_max_length,
                            padding="max_length",
                            truncation=True,
                            return_tensors="pt"
                        ).input_ids.to(self.device)
                        
                        encoder_hidden_states = self.text_encoder(input_ids)[0]
                        
                        # ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆUNet ã¯æ½œåœ¨å¤‰æ•°ã‚’å—ã‘å–ã‚‹ï¼‰
                        latents = self.vae.encode(pixel_values).latent_dist.sample()
                        latents = latents * self.vae.config.scaling_factor
                    
                    # ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(
                        0, noise_scheduler.config.num_train_timesteps,
                        (latents.shape[0],),
                        device=self.device
                    ).long()
                    
                    # ãƒã‚¤ã‚ºãŒè¿½åŠ ã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾
                    noisy_latents = noise_scheduler.add_noise(
                        latents, noise, timesteps
                    )
                    
                    # U-Net äºˆæ¸¬
                    model_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states
                    ).sample
                    
                    # æå¤±è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºäºˆæ¸¬ï¼‰
                    loss = torch.nn.functional.mse_loss(model_pred, noise)
                    
                    # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒƒãƒ—
                    optimizer.zero_grad()
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(self.lora_layers.parameters(), 1.0)
                    optimizer.step()
                    lr_scheduler.step()
                    
                    epoch_loss += loss.item()
                    global_step += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({"loss": f"{loss.item():.6f}"})
                
                avg_loss = epoch_loss / len(dataloader)
                training_log["history"].append({
                    "epoch": epoch + 1,
                    "loss": avg_loss,
                    "lr": optimizer.param_groups[0]["lr"]
                })
                
                print(f"  ğŸ“Š Epoch Loss: {avg_loss:.6f}")
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if (epoch + 1) % save_interval == 0:
                    self._save_checkpoint(epoch + 1)
            
            print("\nâœ… Training completed!")
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°ä¿å­˜
            log_file = self.output_dir / "training_log.json"
            with open(log_file, "w") as f:
                json.dump(training_log, f, indent=2)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.save_lora_weights()
            
            return training_log
        
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            traceback.print_exc()
            raise
    
    def _save_checkpoint(self, epoch: int):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        
        checkpoint_dir = self.output_dir / f"checkpoint-{epoch}"
        checkpoint_dir.mkdir(exist_ok=True)
        
        # LoRA ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿ã‚’ safetensors ã§ä¿å­˜
        state_dict = self.lora_layers.state_dict()
        save_file(state_dict, checkpoint_dir / "lora_weights.safetensors")
        print(f"  ğŸ’¾ Checkpoint saved: {checkpoint_dir}")
    
    def save_lora_weights(self, filename: str = "anime-impressionist-lora.safetensors"):
        """LoRA é‡ã¿ã‚’ SafeTensors ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜
        
        Args:
            filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        
        save_path = self.output_dir / filename
        
        try:
            # LoRA ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚»ãƒƒã‚µã®é‡ã¿ã®ã¿æŠ½å‡ºã—ã¦ä¿å­˜
            state_dict = self.lora_layers.state_dict()
            save_file(state_dict, save_path)
            
            file_size_mb = save_path.stat().st_size / (1024 * 1024)
            print(f"âœ… LoRA weights saved: {save_path} ({file_size_mb:.2f} MB)")
            
            # adapter_config.json ã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            import json as _json
            config = {
                "base_model_name_or_path": self.model_id,
                "lora_rank": self.lora_rank,
                "lora_alpha": self.lora_alpha,
                "target_modules": ["to_k", "to_v", "to_q", "to_out.0"],
                "peft_type": "LORA"
            }
            with open(self.output_dir / "adapter_config.json", "w") as f:
                _json.dump(config, f, indent=2)
            
            return save_path
        
        except Exception as e:
            print(f"âŒ Error saving LoRA weights: {e}")
            traceback.print_exc()
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    parser = argparse.ArgumentParser(
        description="LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="training_data",
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="lora_weights",
        help="LoRA é‡ã¿å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚º (T4: 1-4æ¨å¥¨, A100: 4-8å¯èƒ½)"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="å­¦ç¿’ç‡"
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=8,
        help="LoRA ãƒ©ãƒ³ã‚¯"
    )
    parser.add_argument(
        "--lora_alpha",
        type=float,
        default=32,
        help="LoRA ã‚¢ãƒ«ãƒ•ã‚¡ (ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (cuda, cpu)"
    )
    
    args = parser.parse_args()
    
    # ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (pip install å¾Œ)
    if not IMPORTS_SUCCESS:
        print("âš ï¸  Attempting to install required packages...")
        os.system("pip install -q diffusers transformers accelerate peft pillow torch tqdm safetensors")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å®Ÿè¡Œ
    trainer = LoRATrainer(
        output_dir=args.output_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        device=args.device
    )
    
    training_log = trainer.train(
        data_dir=args.data_dir,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
    
    print("\n" + "="*60)
    print("ğŸ‰ LoRA Training Complete!")
    print("="*60)
    print(f"ğŸ“ Output: {trainer.output_dir}")
    print(f"ğŸ“Š Final Loss: {training_log['history'][-1]['loss']:.6f}")
    print("="*60)


if __name__ == "__main__":
    main()
