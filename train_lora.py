#!/usr/bin/env python3
"""
Anime Impressionist LoRA Training Script

ç”¨é€”:
    Stable Diffusion v1.5 ã« LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨
    Danbooru ã‹ã‚‰åé›†ã—ãŸ 300 æšã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’

å®Ÿè¡Œä¾‹:
    # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ
    python train_lora.py --data_dir training_data --output_dir lora_weights --epochs 50
    
    # Google Colab å®Ÿè¡Œ
    !python train_lora.py --data_dir /content/training_data --output_dir /content/lora_weights --epochs 100

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
    pip install -q diffusers transformers pillow torch tqdm safetensors
"""

import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import traceback

# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç’°å¢ƒä¾å­˜ï¼‰
try:
    from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
    from transformers import CLIPTokenizer, CLIPTextModel
    from safetensors.torch import save_file
    IMPORTS_SUCCESS = True
except ImportError as e:
    print(f"âš ï¸  Some imports failed. Will attempt installation: {e}")
    IMPORTS_SUCCESS = False


class LoRALinear(nn.Module):
    """ç´”ç²‹ PyTorch å®Ÿè£…ã® LoRA ãƒ¬ã‚¤ãƒ¤ãƒ¼
    
    ãƒãƒ¼ã‚¸ãƒ§ãƒ³ä¾å­˜ãªã—: PEFT/diffusers ã®ç‰¹å®š API ã«ä¾å­˜ã—ãªã„
    """
    
    def __init__(self, linear: nn.Linear, rank: int = 8, alpha: float = 32.0):
        super().__init__()
        self.linear = linear
        in_features = linear.in_features
        out_features = linear.out_features
        self.rank = rank
        self.scale = alpha / rank
        
        # LoRA A/B è¡Œåˆ—ï¼ˆå…ƒã® linear ã¨åŒã˜ dtype / deviceï¼‰
        dtype = linear.weight.dtype
        device = linear.weight.device
        self.lora_A = nn.Linear(in_features, rank, bias=False, device=device, dtype=dtype)
        self.lora_B = nn.Linear(rank, out_features, bias=False, device=device, dtype=dtype)
        
        # åˆæœŸåŒ–: A ã¯ Kaiming ä¹±æ•°ã€B ã¯ã‚¼ãƒ­ï¼ˆåˆæœŸå·®åˆ†ã‚¼ãƒ­ï¼‰
        nn.init.kaiming_uniform_(self.lora_A.weight)
        nn.init.zeros_(self.lora_B.weight)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.linear(x) + self.scale * self.lora_B(self.lora_A(x))


def inject_lora_to_unet(unet: nn.Module, rank: int = 8, alpha: float = 32.0) -> list:
    """UNet ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã« LoRA ã‚’æ³¨å…¥
    
    Returns:
        lora_params: å­¦ç¿’å¯¾è±¡ã® LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    unet.requires_grad_(False)
    lora_modules_replaced = 0
    
    for module in unet.modules():
        # CrossAttention / Attention ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã® to_k, to_v, to_q ã‚’ç½®æ›
        for attr in ["to_k", "to_v", "to_q", "to_out"]:
            child = getattr(module, attr, None)
            if child is None:
                continue
            # to_out ã¯ãƒªã‚¹ãƒˆã®ã“ã¨ã‚‚ã‚ã‚‹
            if isinstance(child, nn.ModuleList):
                for i, sub in enumerate(child):
                    if isinstance(sub, nn.Linear):
                        child[i] = LoRALinear(sub, rank=rank, alpha=alpha)
                        lora_modules_replaced += 1
            elif isinstance(child, nn.Linear):
                setattr(module, attr, LoRALinear(child, rank=rank, alpha=alpha))
                lora_modules_replaced += 1
    
    print(f"  ğŸ”§ LoRA æ³¨å…¥: {lora_modules_replaced} modules")
    
    # LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿å­¦ç¿’å¯èƒ½ã«è¨­å®š
    lora_params = []
    for module in unet.modules():
        if isinstance(module, LoRALinear):
            module.lora_A.requires_grad_(True)
            module.lora_B.requires_grad_(True)
            lora_params.extend(list(module.lora_A.parameters()))
            lora_params.extend(list(module.lora_B.parameters()))
    
    return lora_params


def get_lora_state_dict(unet: nn.Module) -> dict:
    """UNet ã‹ã‚‰ LoRA é‡ã¿ã®ã¿æŠ½å‡º"""
    state_dict = {}
    for name, module in unet.named_modules():
        if isinstance(module, LoRALinear):
            state_dict[f"{name}.lora_A.weight"] = module.lora_A.weight.detach().cpu().float()
            state_dict[f"{name}.lora_B.weight"] = module.lora_B.weight.detach().cpu().float()
    return state_dict


class AnimeDataset(Dataset):
    """ã‚¢ãƒ‹ãƒ¡ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, image_size: int = 512):
        """åˆæœŸåŒ–
        
        Args:
            data_dir: training_data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            image_size: å‡ºåŠ›ç”»åƒã‚µã‚¤ã‚º
        """
        self.image_size = image_size
        self.image_paths = []
        self.metadata = {}
        
        data_path = Path(data_dir)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        metadata_file = data_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                self.metadata = json.load(f)
        
        # ç”»åƒãƒ‘ã‚¹ã‚’åé›†
        for style_dir in data_path.iterdir():
            if not style_dir.is_dir() or style_dir.name.startswith("."):
                continue
            
            for img_file in style_dir.glob("*.*"):
                if img_file.suffix.lower() in [".png", ".jpg", ".jpeg"]:
                    self.image_paths.append(str(img_file))
        
        print(f"ğŸ“Š Dataset loaded: {len(self.image_paths)} images")
        
        self.transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size), 
                             interpolation=transforms.InterpolationMode.LANCZOS),
            transforms.CenterCrop(self.image_size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1, 1] ç¯„å›²ã¸æ­£è¦åŒ–
        ])
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® 1 ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
        
        img_path = self.image_paths[idx]
        
        try:
            # ç”»åƒãƒ­ãƒ¼ãƒ‰
            image = Image.open(img_path).convert("RGB")
            pixel_values = self.transform(image)
            
            # ã‚¹ã‚¿ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ï¼‰
            style_name = Path(img_path).parent.name
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆã‚¹ã‚¿ã‚¤ãƒ«å + åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
            prompt = f"{style_name}, anime, masterpiece, high quality"
            
            return {
                "pixel_values": pixel_values,
                "prompt": prompt,
                "file_path": img_path
            }
        
        except Exception as e:
            print(f"âŒ Error loading {img_path}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿”ã™
            return self[0] if idx > 0 else self[(idx + 1) % len(self)]


class LoRATrainer:
    """LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(
        self,
        model_id: str = "runwayml/stable-diffusion-v1-5",
        output_dir: str = "lora_weights",
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        lora_rank: int = 8,
        lora_alpha: float = 32,
    ):
        """åˆæœŸåŒ–
        
        Args:
            model_id: Hugging Face ãƒ¢ãƒ‡ãƒ« ID
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
            lora_rank: LoRA ãƒ©ãƒ³ã‚¯
            lora_alpha: LoRA ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
        """
        self.model_id = model_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.device = device
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        
        print("="*60)
        print("ğŸš€ LoRA Trainer Initialization")
        print("="*60)
        print(f"ğŸ“¦ Model: {model_id}")
        print(f"ğŸ“ Output: {self.output_dir}")
        print(f"ğŸ’» Device: {device}")
        print(f"ğŸ¯ LoRA Config: rank={lora_rank}, alpha={lora_alpha}")
        
        self._setup_model()
    
    def _setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ»LoRA è¨­å®šã®åˆæœŸåŒ– (ç´”ç²‹ PyTorch LoRA)"""
        
        try:
            print("\nğŸ“¥ Loading Stable Diffusion v1.5...")
            dtype = torch.float16 if "cuda" in self.device else torch.float32
            
            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = CLIPTokenizer.from_pretrained(self.model_id, subfolder="tokenizer")
            self.text_encoder = CLIPTextModel.from_pretrained(
                self.model_id, subfolder="text_encoder", torch_dtype=dtype
            ).to(self.device)
            self.vae = AutoencoderKL.from_pretrained(
                self.model_id, subfolder="vae", torch_dtype=dtype
            ).to(self.device)
            self.unet = UNet2DConditionModel.from_pretrained(
                self.model_id, subfolder="unet", torch_dtype=dtype
            ).to(self.device)
            
            # VAE ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯å‡çµ
            self.vae.requires_grad_(False)
            self.text_encoder.requires_grad_(False)
            
            # ç´”ç²‹ PyTorch LoRA ã‚’ UNet ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã«æ³¨å…¥
            # PEFT / diffusers ç‰¹å®š API ã«ä¾å­˜ã—ãªã„ï¼ˆãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›ï¼‰
            self.lora_params = inject_lora_to_unet(
                self.unet, rank=self.lora_rank, alpha=self.lora_alpha
            )
            
            # å®‰å…¨ç­–: LoRA æ³¨å…¥å¾Œã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«ãƒ‡ãƒã‚¤ã‚¹ã¸ç§»å‹•
            self.unet.to(self.device)
            # lora_params ã®å‚ç…§ã‚‚æ›´æ–°
            self.lora_params = []
            for m in self.unet.modules():
                if isinstance(m, LoRALinear):
                    self.lora_params.extend(list(m.lora_A.parameters()))
                    self.lora_params.extend(list(m.lora_B.parameters()))
            
            total_params = sum(p.numel() for p in self.lora_params)
            print(f"âœ… LoRA configured: {total_params:,} trainable params")
            print(f"âœ… Model loaded (ç´”ç²‹ PyTorch LoRA, ãƒãƒ¼ã‚¸ãƒ§ãƒ³éä¾å­˜)")
            
        except Exception as e:
            print(f"âŒ Error setting up model: {e}")
            traceback.print_exc()
            raise
    
    def train(
        self,
        data_dir: str,
        num_epochs: int = 50,
        batch_size: int = 1,
        learning_rate: float = 1e-4,
        num_workers: int = 0,
        save_interval: int = 5,
    ):
        """LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        
        Args:
            data_dir: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            num_workers: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            save_interval: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰
        """
        
        print("\n" + "="*60)
        print("ğŸ“ Starting LoRA Training")
        print("="*60)
        print(f"ğŸ“Š Training Epochs: {num_epochs}")
        print(f"ğŸ“¦ Batch Size: {batch_size}")
        print(f"ğŸ¯ Learning Rate: {learning_rate}")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dataset = AnimeDataset(data_dir)
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers
            )
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š (LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿)
            optimizer = torch.optim.AdamW(
                self.lora_params,
                lr=learning_rate
            )
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            num_training_steps = len(dataloader) * num_epochs
            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, num_training_steps
            )
            
            # ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
            noise_scheduler = DDPMScheduler.from_pretrained(
                self.model_id, subfolder="scheduler"
            )
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            self.unet.train()   # UNet ã¯ train ãƒ¢ãƒ¼ãƒ‰ï¼ˆLoRAã®ã¿å‹¾é…ã‚ã‚Šï¼‰
            self.vae.eval()
            self.text_encoder.eval()
            # LoRA ä»¥å¤–ã® UNet ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‹¾é…ãªã—ï¼ˆinject æ™‚ã«è¨­å®šæ¸ˆã¿ï¼‰
            
            training_log = {
                "config": {
                    "model_id": self.model_id,
                    "num_epochs": num_epochs,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "lora_rank": self.lora_rank,
                    "lora_alpha": self.lora_alpha,
                },
                "history": []
            }
            
            global_step = 0
            
            for epoch in range(num_epochs):
                print(f"\n[Epoch {epoch + 1}/{num_epochs}]")
                epoch_loss = 0.0
                
                pbar = tqdm(dataloader, desc="Training", leave=False)
                
                for batch_idx, batch in enumerate(pbar):
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                    prompts = batch["prompt"]
                    dtype = torch.float16 if "cuda" in self.device else torch.float32
                    pixel_values = batch["pixel_values"].to(device=self.device, dtype=dtype)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ & VAE ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå‹¾é…ä¸è¦ï¼‰
                    with torch.no_grad():
                        input_ids = self.tokenizer(
                            prompts,
                            max_length=self.tokenizer.model_max_length,
                            padding="max_length",
                            truncation=True,
                            return_tensors="pt"
                        ).input_ids.to(self.device)
                        
                        encoder_hidden_states = self.text_encoder(input_ids)[0]
                        
                        # ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆUNet ã¯æ½œåœ¨å¤‰æ•°ã‚’å—ã‘å–ã‚‹ï¼‰
                        latents = self.vae.encode(pixel_values).latent_dist.sample()
                        latents = latents * self.vae.config.scaling_factor
                    
                    # ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(
                        0, noise_scheduler.config.num_train_timesteps,
                        (latents.shape[0],),
                        device=self.device
                    ).long()
                    
                    # ãƒã‚¤ã‚ºãŒè¿½åŠ ã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾
                    noisy_latents = noise_scheduler.add_noise(
                        latents, noise, timesteps
                    )
                    
                    # U-Net äºˆæ¸¬
                    model_pred = self.unet(
                        noisy_latents,
                        timesteps,
                        encoder_hidden_states
                    ).sample
                    
                    # æå¤±è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºäºˆæ¸¬ï¼‰
                    loss = torch.nn.functional.mse_loss(model_pred, noise)
                    
                    # NaN ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
                    if torch.isnan(loss):
                        print(f"    âš ï¸  NaN detected at step {global_step}, skipping batch")
                        optimizer.zero_grad()
                        continue
                    
                    # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒƒãƒ—
                    optimizer.zero_grad()
                    loss.backward()
                    
                    # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼†ãƒã‚§ãƒƒã‚¯
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.lora_params, 1.0)
                    if grad_norm > 10.0:
                        print(f"    âš ï¸  High gradient norm: {grad_norm:.4f} at step {global_step}")
                    
                    optimizer.step()
                    lr_scheduler.step()
                    
                    epoch_loss += loss.item()
                    global_step += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({"loss": f"{loss.item():.6f}"})
                
                avg_loss = epoch_loss / len(dataloader) if len(dataloader) > 0 else float('nan')
                training_log["history"].append({
                    "epoch": epoch + 1,
                    "loss": avg_loss,
                    "lr": optimizer.param_groups[0]["lr"]
                })
                
                if torch.isnan(torch.tensor(avg_loss)):
                    print(f"  âš ï¸  Epoch Loss: nan (potential training instability)")
                else:
                    print(f"  ğŸ“Š Epoch Loss: {avg_loss:.6f}")
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if (epoch + 1) % save_interval == 0:
                    self._save_checkpoint(epoch + 1)
            
            print("\nâœ… Training completed!")
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°ä¿å­˜
            log_file = self.output_dir / "training_log.json"
            with open(log_file, "w") as f:
                json.dump(training_log, f, indent=2)
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.save_lora_weights()
            
            return training_log
        
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            traceback.print_exc()
            raise
    
    def _save_checkpoint(self, epoch: int):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        
        checkpoint_dir = self.output_dir / f"checkpoint-{epoch}"
        checkpoint_dir.mkdir(exist_ok=True)
        
        state_dict = get_lora_state_dict(self.unet)
        save_file(state_dict, checkpoint_dir / "lora_weights.safetensors")
        print(f"  ğŸ’¾ Checkpoint saved: {checkpoint_dir} ({len(state_dict)} tensors)")
    
    def save_lora_weights(self, filename: str = "anime-impressionist-lora.safetensors"):
        """LoRA é‡ã¿ã‚’ SafeTensors ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜
        
        Args:
            filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        
        save_path = self.output_dir / filename
        
        try:
            state_dict = get_lora_state_dict(self.unet)
            save_file(state_dict, save_path)
            
            file_size_mb = save_path.stat().st_size / (1024 * 1024)
            print(f"âœ… LoRA weights saved: {save_path} ({file_size_mb:.2f} MB, {len(state_dict)} tensors)")
            
            # adapter_config.json ã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            config = {
                "base_model_name_or_path": self.model_id,
                "lora_rank": self.lora_rank,
                "lora_alpha": float(self.lora_alpha),
                "target_modules": ["to_k", "to_v", "to_q", "to_out"],
                "implementation": "pytorch_native"
            }
            with open(self.output_dir / "adapter_config.json", "w") as f:
                json.dump(config, f, indent=2)
            
            return save_path
        
        except Exception as e:
            print(f"âŒ Error saving LoRA weights: {e}")
            traceback.print_exc()
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    parser = argparse.ArgumentParser(
        description="LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="training_data",
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="lora_weights",
        help="LoRA é‡ã¿å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚º (T4: 1-4æ¨å¥¨, A100: 4-8å¯èƒ½)"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="å­¦ç¿’ç‡"
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=8,
        help="LoRA ãƒ©ãƒ³ã‚¯"
    )
    parser.add_argument(
        "--lora_alpha",
        type=float,
        default=32,
        help="LoRA ã‚¢ãƒ«ãƒ•ã‚¡ (ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (cuda, cpu)"
    )
    
    args = parser.parse_args()
    
    # ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (pip install å¾Œ)
    if not IMPORTS_SUCCESS:
        print("âš ï¸  Attempting to install required packages...")
        os.system("pip install -q diffusers transformers pillow torch tqdm safetensors")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å®Ÿè¡Œ
    trainer = LoRATrainer(
        output_dir=args.output_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        device=args.device
    )
    
    training_log = trainer.train(
        data_dir=args.data_dir,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
    
    print("\n" + "="*60)
    print("ğŸ‰ LoRA Training Complete!")
    print("="*60)
    print(f"ğŸ“ Output: {trainer.output_dir}")
    print(f"ğŸ“Š Final Loss: {training_log['history'][-1]['loss']:.6f}")
    print("="*60)


if __name__ == "__main__":
    main()
