#!/usr/bin/env python3
"""
Anime Impressionist LoRA Training Script

ç”¨é€”:
    Stable Diffusion v1.5 ã« LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨
    Danbooru ã‹ã‚‰åé›†ã—ãŸ 300 æšã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’

å®Ÿè¡Œä¾‹:
    # ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œ
    python train_lora.py --data_dir training_data --output_dir lora_weights --epochs 50
    
    # Google Colab å®Ÿè¡Œ
    !python train_lora.py --data_dir /content/training_data --output_dir /content/lora_weights --epochs 100

ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
    pip install -q diffusers transformers pillow torch tqdm safetensors
"""

import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
from tqdm import tqdm
import traceback

# æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç’°å¢ƒä¾å­˜ï¼‰
try:
    from diffusers import DDPMScheduler, AutoencoderKL, UNet2DConditionModel
    from transformers import CLIPTokenizer, CLIPTextModel
    from safetensors.torch import save_file
    IMPORTS_SUCCESS = True
except ImportError as e:
    print(f"âš ï¸  Some imports failed. Will attempt installation: {e}")
    IMPORTS_SUCCESS = False


class DoRALinear(nn.Module):
    """
    DoRA (Dominant Rank Adaptation) + QLoRA å¯¾å¿œã®ç´”ç²‹ PyTorch å®Ÿè£…
    
    DoRA: magnitude (ã‚¹ã‚±ãƒ¼ãƒ«) ã¨ direction (æ–¹å‘) ã‚’åˆ†é›¢
    - magnitude: ä½å‘¨æ³¢æˆåˆ†ï¼ˆã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´ï¼‰
    - direction: é«˜å‘¨æ³¢æˆåˆ†ï¼ˆç´°éƒ¨ç‰¹æ€§ï¼‰
    
    QLoRAå¯¾å¿œï¼š
    - direction ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ä½ç²¾åº¦ï¼ˆint8/fp4ç›¸å½“ï¼‰ã§ä¿æŒ â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    - magnitude ã¯é«˜ç²¾åº¦ã®ã¾ã¾ï¼ˆå­¦ç¿’ã®å®‰å®šæ€§é‡è¦–ï¼‰
    - é †ä¼æ’­: direction ã¯æ··åˆç²¾åº¦ã§è¨ˆç®—
    
    å‚è€ƒ: 
    - DoRA: https://arxiv.org/abs/2402.09353
    - QLoRA: https://arxiv.org/abs/2305.14314
    """
    
    def __init__(self, linear: nn.Linear, rank: int = 32, alpha: float = 32.0, use_qlora: bool = False):
        super().__init__()
        self.linear = linear
        in_features = linear.in_features
        out_features = linear.out_features
        self.rank = rank
        self.scale = alpha / rank
        self.use_qlora = use_qlora
        
        # ãƒ‡ãƒã‚¤ã‚¹ãƒ»dtype ã‚’å…ƒãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨çµ±ä¸€
        dtype = linear.weight.dtype
        device = linear.weight.device
        
        # â‘  Magnitude vector (ä½å‘¨æ³¢æˆåˆ†: ã‚¹ã‚±ãƒ¼ãƒ«)
        # å¸¸ã«é«˜ç²¾åº¦ï¼ˆå­¦ç¿’ã®å®‰å®šæ€§ï¼‰
        self.magnitude = nn.Parameter(
            torch.zeros(out_features, dtype=dtype, device=device)
        )
        
        # â‘¡ Direction matrix (é«˜å‘¨æ³¢æˆåˆ†: å¾®ç´°ãªç‰¹æ€§)
        # QLoRA: direction ã¯ä½ç²¾åº¦ã§ä¿æŒã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
        direction_dtype = torch.float16 if use_qlora else dtype
        self.direction_a = nn.Linear(in_features, rank, bias=False, device=device, dtype=direction_dtype)
        self.direction_b = nn.Linear(rank, out_features, bias=False, device=device, dtype=direction_dtype)
        
        # åˆæœŸåŒ–
        nn.init.kaiming_uniform_(self.direction_a.weight, a=0.01)
        nn.init.zeros_(self.direction_b.weight)
        nn.init.zeros_(self.magnitude)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        DoRA (+ QLoRA) forward pass:
        y = W_base(x) + scale * (1 + magnitude) * direction_b(direction_a(x))
        
        QLoRA æ™‚: direction ã¯ float16 ã§è¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
        """
        # ãƒ™ãƒ¼ã‚¹é‡ã¿ã«ã‚ˆã‚‹å‡ºåŠ›
        base_out = self.linear(x)
        
        # Direction component: ä½ãƒ©ãƒ³ã‚¯åˆ†è§£
        # QLoRA: x ã¯å…ƒã®ç²¾åº¦ã€direction_a ã‹ã‚‰ç•°ãªã‚‹ç²¾åº¦ã§è¿”ã•ã‚Œã‚‹
        if self.use_qlora:
            # float16 direction ã§è¨ˆç®—
            intermediate = self.direction_a(x.to(self.direction_a.weight.dtype))
            direction_delta = self.direction_b(intermediate).to(x.dtype)
        else:
            # é€šå¸¸: åŒä¸€ç²¾åº¦
            direction_delta = self.direction_b(self.direction_a(x))
        
        # Magnitude component: ã‚¹ã‚±ãƒ¼ãƒ«é©ç”¨
        magnitude_scaled = (1.0 + self.magnitude) * direction_delta
        
        return base_out + self.scale * magnitude_scaled


# LoRA äº’æ›æ€§ã®ãŸã‚ã« LoRALinear ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ä¿æŒ
LoRALinear = DoRALinear


def inject_lora_to_unet(unet: nn.Module, rank: int = 32, alpha: float = 32.0, use_qlora: bool = False) -> list:
    """UNet ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã« DoRA (+ QLoRA) ã‚’æ³¨å…¥
    
    DoRA (Dominant Rank Adaptation):
    - magnitude: ä½å‘¨æ³¢ã‚¹ã‚±ãƒ¼ãƒ«æˆåˆ†ï¼ˆç”»åƒã®å¤§åŸŸçš„ãªç‰¹æ€§ï¼‰
    - direction: é«˜å‘¨æ³¢æ–¹å‘æˆåˆ†ï¼ˆç´°éƒ¨ãƒã‚¤ã‚ºãƒ»ãƒ†ã‚¯ã‚¹ãƒãƒ£ï¼‰
    
    QLoRA (Quantized LoRA):
    - direction ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ float16 ã§é‡å­åŒ– â†’ ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    - magnitude ã¯é«˜ç²¾åº¦ã®ã¾ã¾ï¼ˆå­¦ç¿’å®‰å®šæ€§ï¼‰
    
    Args:
        rank: DoRA ãƒ©ãƒ³ã‚¯ï¼ˆæ¨å¥¨: 32-64 ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿæ¸¬æœ€é©å€¤ï¼‰
        alpha: ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
        use_qlora: QLoRA ã‚’æœ‰åŠ¹åŒ–ï¼ˆãƒ¡ãƒ¢ãƒªã‚»ãƒ¼ãƒãƒ¼ã€ç²¾åº¦-ãƒ¡ãƒ¢ãƒªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰
    
    Returns:
        lora_params: å­¦ç¿’å¯¾è±¡ã® DoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
    """
    unet.requires_grad_(False)
    lora_modules_replaced = 0
    
    for module in unet.modules():
        # CrossAttention / Attention ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã® to_k, to_v, to_q ã‚’ç½®æ›
        for attr in ["to_k", "to_v", "to_q", "to_out"]:
            child = getattr(module, attr, None)
            if child is None:
                continue
            # to_out ã¯ãƒªã‚¹ãƒˆã®ã“ã¨ã‚‚ã‚ã‚‹
            if isinstance(child, nn.ModuleList):
                for i, sub in enumerate(child):
                    if isinstance(sub, nn.Linear):
                        child[i] = DoRALinear(sub, rank=rank, alpha=alpha, use_qlora=use_qlora)
                        lora_modules_replaced += 1
            elif isinstance(child, nn.Linear):
                setattr(module, attr, DoRALinear(child, rank=rank, alpha=alpha, use_qlora=use_qlora))
                lora_modules_replaced += 1
    
    print(f"  ğŸ”§ LoRA æ³¨å…¥: {lora_modules_replaced} modules")
    
    # LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿å­¦ç¿’å¯èƒ½ã«è¨­å®š
    lora_params = []
    for module in unet.modules():
        if isinstance(module, DoRALinear):
            module.magnitude.requires_grad_(True)
            module.direction_a.requires_grad_(True)
            module.direction_b.requires_grad_(True)
            lora_params.append(module.magnitude)
            lora_params.extend(list(module.direction_a.parameters()))
            lora_params.extend(list(module.direction_b.parameters()))
    
    return lora_params


def get_lora_state_dict(unet: nn.Module) -> dict:
    """UNet ã‹ã‚‰ DoRA é‡ã¿ã®ã¿æŠ½å‡º"""
    state_dict = {}
    for name, module in unet.named_modules():
        if isinstance(module, DoRALinear):
            state_dict[f"{name}.magnitude"] = module.magnitude.detach().cpu().float()
            state_dict[f"{name}.direction_a.weight"] = module.direction_a.weight.detach().cpu().float()
            state_dict[f"{name}.direction_b.weight"] = module.direction_b.weight.detach().cpu().float()
    return state_dict


class AnimeDataset(Dataset):
    """ã‚¢ãƒ‹ãƒ¡ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, data_dir: str, image_size: int = 512):
        """åˆæœŸåŒ–
        
        Args:
            data_dir: training_data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            image_size: å‡ºåŠ›ç”»åƒã‚µã‚¤ã‚º
        """
        self.image_size = image_size
        self.image_paths = []
        self.metadata = {}
        
        data_path = Path(data_dir)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
        metadata_file = data_path / "metadata.json"
        if metadata_file.exists():
            with open(metadata_file, "r", encoding="utf-8") as f:
                self.metadata = json.load(f)
        
        # ç”»åƒãƒ‘ã‚¹ã‚’åé›†
        for style_dir in data_path.iterdir():
            if not style_dir.is_dir() or style_dir.name.startswith("."):
                continue
            
            for img_file in style_dir.glob("*.*"):
                if img_file.suffix.lower() in [".png", ".jpg", ".jpeg"]:
                    self.image_paths.append(str(img_file))
        
        print(f"ğŸ“Š Dataset loaded: {len(self.image_paths)} images")
        
        self.transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size), 
                             interpolation=transforms.InterpolationMode.LANCZOS),
            transforms.CenterCrop(self.image_size),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])  # [-1, 1] ç¯„å›²ã¸æ­£è¦åŒ–
        ])
    
    def __len__(self) -> int:
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® 1 ã‚µãƒ³ãƒ—ãƒ«ã‚’å–å¾—"""
        
        img_path = self.image_paths[idx]
        
        try:
            # ç”»åƒãƒ­ãƒ¼ãƒ‰
            image = Image.open(img_path).convert("RGB")
            pixel_values = self.transform(image)
            
            # ã‚¹ã‚¿ã‚¤ãƒ«åã‚’å–å¾—ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‹ã‚‰ï¼‰
            style_name = Path(img_path).parent.name
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆã‚¹ã‚¿ã‚¤ãƒ«å + åŸºæœ¬ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
            prompt = f"{style_name}, anime, masterpiece, high quality"
            
            return {
                "pixel_values": pixel_values,
                "prompt": prompt,
                "file_path": img_path
            }
        
        except Exception as e:
            print(f"âŒ Error loading {img_path}: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿”ã™
            return self[0] if idx > 0 else self[(idx + 1) % len(self)]


class LoRATrainer:
    """LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼"""
    
    def __init__(
        self,
        model_id: str = "runwayml/stable-diffusion-v1-5",
        output_dir: str = "lora_weights",
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        lora_rank: int = 32,
        lora_alpha: float = 32,
        use_qlora: bool = False,
    ):
        """åˆæœŸåŒ–
        
        Args:
            model_id: Hugging Face ãƒ¢ãƒ‡ãƒ« ID
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            device: è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹
            lora_rank: DoRA ãƒ©ãƒ³ã‚¯ï¼ˆæ¨å¥¨: 32-64ï¼‰
            lora_alpha: DoRA ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°
            use_qlora: QLoRA ã‚’ä½¿ç”¨ï¼ˆdirectioné‡å­åŒ–ã€ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼‰
        """
        self.model_id = model_id
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.device = device
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.use_qlora = use_qlora
        
        print("="*60)
        print("ğŸš€ DoRA Trainer Initialization")
        print("="*60)
        print(f"ğŸ“¦ Model: {model_id}")
        print(f"ğŸ“ Output: {self.output_dir}")
        print(f"ğŸ’» Device: {device}")
        print(f"ğŸ¯ DoRA Config: rank={lora_rank}, alpha={lora_alpha}")
        if use_qlora:
            print(f"âš¡ QLoRA: Enabled (direction: float16 â†’ int8 quantization)")
        print(f"   (Dominant Rank Adaptation - magnitude + direction decomposition)")
        print(f"ğŸ’» Device: {device}")
        print(f"ğŸ¯ DoRA Config: rank={lora_rank}, alpha={lora_alpha}")
        print(f"   (Dominant Rank Adaptation - magnitude + direction decomposition)")
        
        self._setup_model()
    
    def _setup_model(self):
        """ãƒ¢ãƒ‡ãƒ«ãƒ»DoRA è¨­å®šã®åˆæœŸåŒ– (ç´”ç²‹ PyTorch DoRA)"""
        
        try:
            print("\nğŸ“¥ Loading Stable Diffusion v1.5...")
            dtype = torch.float16 if "cuda" in self.device else torch.float32
            
            # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å€‹åˆ¥ã«ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = CLIPTokenizer.from_pretrained(self.model_id, subfolder="tokenizer")
            self.text_encoder = CLIPTextModel.from_pretrained(
                self.model_id, subfolder="text_encoder", torch_dtype=dtype
            ).to(self.device)
            self.vae = AutoencoderKL.from_pretrained(
                self.model_id, subfolder="vae", torch_dtype=dtype
            ).to(self.device)
            self.unet = UNet2DConditionModel.from_pretrained(
                self.model_id, subfolder="unet", torch_dtype=dtype
            ).to(self.device)
            
            # VAE ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¯å‡çµ
            self.vae.requires_grad_(False)
            self.text_encoder.requires_grad_(False)
            
            # ç´”ç²‹ PyTorch DoRA ã‚’ UNet ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã«æ³¨å…¥
            # Magnitude (ä½å‘¨æ³¢) ã¨ Direction (é«˜å‘¨æ³¢) ã‚’åˆ†é›¢å­¦ç¿’
            self.lora_params = inject_lora_to_unet(
                self.unet, rank=self.lora_rank, alpha=self.lora_alpha, use_qlora=self.use_qlora
            )
            
            # å®‰å…¨ç­–: DoRA æ³¨å…¥å¾Œã«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºå®Ÿã«ãƒ‡ãƒã‚¤ã‚¹ã¸ç§»å‹•
            self.unet.to(self.device)
            
            # Gradient Checkpointing æœ‰åŠ¹åŒ–ï¼ˆVRAM ç¯€ç´„ï¼‰
            if "cuda" in self.device:
                self.unet.enable_gradient_checkpointing()
                print("âœ… Gradient checkpointing enabled (VRAM ç¯€ç´„)")
            
            # DoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å‚ç…§ã‚’æ›´æ–°ï¼ˆmagnitude + direction ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼‰
            self.lora_params = []
            for m in self.unet.modules():
                if isinstance(m, DoRALinear):
                    self.lora_params.append(m.magnitude)  # magnitude ãƒ™ã‚¯ãƒˆãƒ«
                    self.lora_params.extend(list(m.direction_a.parameters()))
                    self.lora_params.extend(list(m.direction_b.parameters()))
            
            total_params = sum(p.numel() for p in self.lora_params)
            print(f"âœ… DoRA configured: {total_params:,} trainable params")
            if self.use_qlora:
                print(f"âœ… QLoRA enabled: direction components in float16 (memory efficient)")
            print(f"âœ… Model loaded (ç´”ç²‹ PyTorch DoRA, ä½å‘¨æ³¢Ã—é«˜å‘¨æ³¢åˆ†é›¢å­¦ç¿’)")
            
        except Exception as e:
            print(f"âŒ Error setting up model: {e}")
            traceback.print_exc()
            raise
    
    def train(
        self,
        data_dir: str,
        num_epochs: int = 50,
        batch_size: int = 1,
        learning_rate: float = 1e-4,
        num_workers: int = 0,
        save_interval: int = 5,
        use_qlora: bool = True,
        gradient_accumulation_steps: int = 1,
    ):
        """LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
        
        Args:
            data_dir: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            num_epochs: ã‚¨ãƒãƒƒã‚¯æ•°
            batch_size: ãƒãƒƒãƒã‚µã‚¤ã‚º
            learning_rate: å­¦ç¿’ç‡
            num_workers: ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
            save_interval: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜é–“éš”ï¼ˆã‚¨ãƒãƒƒã‚¯ï¼‰
            use_qlora: QLoRA ã‚’æœ‰åŠ¹åŒ–
            gradient_accumulation_steps: å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—æ•°ï¼ˆå®ŸåŠ¹ãƒãƒƒãƒ = batch_size * stepsï¼‰
        """
        
        print("\n" + "="*60)
        print("ğŸ“ Starting DoRA Training")
        print("="*60)
        print(f"ğŸ“Š Training Epochs: {num_epochs}")
        print(f"ğŸ“¦ Batch Size: {batch_size}")
        print(f"ğŸ“¦ Gradient Accumulation: {gradient_accumulation_steps} steps (effective batch={batch_size * gradient_accumulation_steps})")
        print(f"ğŸ¯ Learning Rate: {learning_rate}")
        print(f"ğŸ’¡ DoRA: Magnitude (ä½å‘¨æ³¢) + Direction (é«˜å‘¨æ³¢) åˆ†é›¢å­¦ç¿’")
        print(f"ğŸ”§ QLoRA: {use_qlora}")
        use_amp = "cuda" in self.device
        if use_amp:
            print(f"âš¡ Mixed Precision (AMP): Enabled")
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°ã®åˆæœŸåŒ–ï¼ˆtry-except å¤–ï¼‰
        training_log = {
            "config": {
                "model_id": self.model_id,
                "num_epochs": num_epochs,
                "batch_size": batch_size,
                "gradient_accumulation_steps": gradient_accumulation_steps,
                "effective_batch_size": batch_size * gradient_accumulation_steps,
                "learning_rate": learning_rate,
                "lora_rank": self.lora_rank,
                "lora_alpha": self.lora_alpha,
                "use_qlora": use_qlora,
                "mixed_precision": use_amp,
            },
            "history": [],
            "status": "initializing"
        }
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
            dataset = AnimeDataset(data_dir)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¤œè¨¼
            if len(dataset) == 0:
                raise ValueError(
                    f"âŒ Dataset is empty! No images found in {data_dir}\n"
                    "   Expected structure: {data_dir}/<style_name>/*.png\n"
                    "   Please upload training images first."
                )
            
            print(f"âœ… Dataset loaded: {len(dataset)} images")
            
            dataloader = DataLoader(
                dataset,
                batch_size=batch_size,
                shuffle=True,
                num_workers=num_workers
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æ¤œè¨¼
            num_batches = len(dataloader)
            print(f"âœ… DataLoader ready: {num_batches} batches")
            
            if num_batches == 0:
                raise ValueError(
                    f"âŒ DataLoader has 0 batches!\n"
                    f"   Dataset size: {len(dataset)}, Batch size: {batch_size}\n"
                    "   This usually means the DataLoader failed to create batches."
                )
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼è¨­å®š (LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿)
            optimizer = torch.optim.AdamW(
                self.lora_params,
                lr=learning_rate
            )
            
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
            num_training_steps = num_batches * num_epochs
            lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, num_training_steps
            )
            
            # ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
            noise_scheduler = DDPMScheduler.from_pretrained(
                self.model_id, subfolder="scheduler"
            )
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            self.unet.train()   # UNet ã¯ train ãƒ¢ãƒ¼ãƒ‰ï¼ˆLoRAã®ã¿å‹¾é…ã‚ã‚Šï¼‰
            self.vae.eval()
            self.text_encoder.eval()
            # LoRA ä»¥å¤–ã® UNet ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å‹¾é…ãªã—ï¼ˆinject æ™‚ã«è¨­å®šæ¸ˆã¿ï¼‰
            
            # Mixed Precision (AMP) è¨­å®š
            scaler = torch.amp.GradScaler("cuda", enabled=use_amp) if use_amp else None
            
            # æ—¢ã«å®šç¾©ã•ã‚ŒãŸ training_log ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±ã‚’è¿½åŠ 
            training_log["config"]["dataset_size"] = len(dataset)
            training_log["config"]["num_batches"] = num_batches
            training_log["status"] = "training"
            
            global_step = 0
            
            for epoch in range(num_epochs):
                print(f"\n[Epoch {epoch + 1}/{num_epochs}]")
                epoch_loss = 0.0
                num_valid_batches = 0
                
                pbar = tqdm(dataloader, desc="Training", leave=False)
                
                for batch_idx, batch in enumerate(pbar):
                    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
                    prompts = batch["prompt"]
                    dtype = torch.float16 if "cuda" in self.device else torch.float32
                    pixel_values = batch["pixel_values"].to(device=self.device, dtype=dtype)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ & VAE ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆå‹¾é…ä¸è¦ï¼‰
                    with torch.no_grad():
                        input_ids = self.tokenizer(
                            prompts,
                            max_length=self.tokenizer.model_max_length,
                            padding="max_length",
                            truncation=True,
                            return_tensors="pt"
                        ).input_ids.to(self.device)
                        
                        encoder_hidden_states = self.text_encoder(input_ids)[0]
                        
                        # ç”»åƒã‚’æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆUNet ã¯æ½œåœ¨å¤‰æ•°ã‚’å—ã‘å–ã‚‹ï¼‰
                        latents = self.vae.encode(pixel_values).latent_dist.sample()
                        latents = latents * self.vae.config.scaling_factor
                    
                    # VAE å‡ºåŠ›å¾Œã«ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã‚’è§£æ”¾
                    del pixel_values, input_ids
                    
                    # ãƒã‚¤ã‚ºã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
                    noise = torch.randn_like(latents)
                    timesteps = torch.randint(
                        0, noise_scheduler.config.num_train_timesteps,
                        (latents.shape[0],),
                        device=self.device
                    ).long()
                    
                    # ãƒã‚¤ã‚ºãŒè¿½åŠ ã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾
                    noisy_latents = noise_scheduler.add_noise(
                        latents, noise, timesteps
                    )
                    
                    # U-Net äºˆæ¸¬ (AMP autocast ã§ forward pass ã‚’ float16 è¨ˆç®—)
                    if use_amp:
                        with torch.amp.autocast("cuda", dtype=torch.float16):
                            model_pred = self.unet(
                                noisy_latents,
                                timesteps,
                                encoder_hidden_states
                            ).sample
                            # æå¤±è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºäºˆæ¸¬ï¼‰
                            loss = torch.nn.functional.mse_loss(model_pred.float(), noise.float())
                    else:
                        model_pred = self.unet(
                            noisy_latents,
                            timesteps,
                            encoder_hidden_states
                        ).sample
                        loss = torch.nn.functional.mse_loss(model_pred, noise)
                    
                    # NaN ãƒã‚§ãƒƒã‚¯ï¼ˆæ•°å€¤å®‰å®šæ€§ï¼‰
                    if torch.isnan(loss):
                        print(f"    âš ï¸  NaN detected at step {global_step}, skipping batch")
                        optimizer.zero_grad()
                        continue
                    
                    # å‹¾é…è“„ç©ã®ãŸã‚ã«ã‚¹ã‚±ãƒ¼ãƒ«
                    loss = loss / gradient_accumulation_steps
                    
                    # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒƒãƒ— (AMP scaler)
                    if use_amp:
                        scaler.scale(loss).backward()
                    else:
                        loss.backward()
                    
                    # å‹¾é…è“„ç©ã‚¹ãƒ†ãƒƒãƒ—å®Œäº†æ™‚ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°
                    if (batch_idx + 1) % gradient_accumulation_steps == 0 or (batch_idx + 1) == len(dataloader):
                        if use_amp:
                            scaler.unscale_(optimizer)
                        
                        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ï¼†ãƒã‚§ãƒƒã‚¯
                        grad_norm = torch.nn.utils.clip_grad_norm_(self.lora_params, 1.0)
                        if grad_norm > 10.0:
                            print(f"    âš ï¸  High gradient norm: {grad_norm:.4f} at step {global_step}")
                        
                        if use_amp:
                            scaler.step(optimizer)
                            scaler.update()
                        else:
                            optimizer.step()
                        
                        lr_scheduler.step()
                        optimizer.zero_grad()
                    
                    epoch_loss += loss.item() * gradient_accumulation_steps  # unscale for logging
                    num_valid_batches += 1
                    global_step += 1
                    
                    pbar.update(1)
                    pbar.set_postfix({"loss": f"{loss.item():.6f}"})
                
                avg_loss = epoch_loss / num_valid_batches if num_valid_batches > 0 else float('nan')
                training_log["history"].append({
                    "epoch": epoch + 1,
                    "loss": avg_loss,
                    "lr": optimizer.param_groups[0]["lr"]
                })
                
                if torch.isnan(torch.tensor(avg_loss)):
                    print(f"  âš ï¸  Epoch Loss: nan (potential training instability)")
                else:
                    print(f"  ğŸ“Š Epoch Loss: {avg_loss:.6f}")
                
                # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if (epoch + 1) % save_interval == 0:
                    self._save_checkpoint(epoch + 1)
            
            print("\nâœ… Training completed!")
            training_log["status"] = "completed"
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ­ã‚°ä¿å­˜
            log_file = self.output_dir / "training_log.json"
            with open(log_file, "w") as f:
                json.dump(training_log, f, indent=2)
            print(f"ğŸ“Š Training log saved: {log_file}")
            
            # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
            self.save_lora_weights()
            
            return training_log
        
        except Exception as e:
            print(f"\nâŒ Training error: {e}")
            traceback.print_exc()
            
            # ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°ã«è¿½åŠ ã—ã¦ä¿å­˜ï¼ˆã§ãã‚‹ãªã‚‰ï¼‰
            try:
                training_log["status"] = "error"
                training_log["error"] = str(e)
                training_log["error_traceback"] = traceback.format_exc()
                
                self.output_dir.mkdir(parents=True, exist_ok=True)
                log_file = self.output_dir / "training_log.json"
                with open(log_file, "w") as f:
                    json.dump(training_log, f, indent=2)
                print(f"ğŸ“Š Error log saved: {log_file}")
            except Exception as log_error:
                print(f"âš ï¸  Could not save error log: {log_error}")
            
            raise
    
    def _save_checkpoint(self, epoch: int):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        
        checkpoint_dir = self.output_dir / f"checkpoint-{epoch}"
        checkpoint_dir.mkdir(exist_ok=True)
        
        state_dict = get_lora_state_dict(self.unet)
        save_file(state_dict, checkpoint_dir / "lora_weights.safetensors")
        print(f"  ğŸ’¾ Checkpoint saved: {checkpoint_dir} ({len(state_dict)} tensors)")
    
    def save_lora_weights(self, filename: str = "anime-impressionist-lora.safetensors"):
        """LoRA é‡ã¿ã‚’ SafeTensors ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¿å­˜
        
        Args:
            filename: ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        
        save_path = self.output_dir / filename
        
        try:
            state_dict = get_lora_state_dict(self.unet)
            save_file(state_dict, save_path)
            
            file_size_mb = save_path.stat().st_size / (1024 * 1024)
            print(f"âœ… LoRA weights saved: {save_path} ({file_size_mb:.2f} MB, {len(state_dict)} tensors)")
            
            # adapter_config.json ã‚‚ä¿å­˜ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
            config = {
                "base_model_name_or_path": self.model_id,
                "lora_rank": self.lora_rank,
                "lora_alpha": float(self.lora_alpha),
                "target_modules": ["to_k", "to_v", "to_q", "to_out"],
                "implementation": "pytorch_native"
            }
            with open(self.output_dir / "adapter_config.json", "w") as f:
                json.dump(config, f, indent=2)
            
            return save_path
        
        except Exception as e:
            print(f"âŒ Error saving LoRA weights: {e}")
            traceback.print_exc()
            raise


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    
    parser = argparse.ArgumentParser(
        description="LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ"
    )
    parser.add_argument(
        "--data_dir",
        type=str,
        default="training_data",
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="lora_weights",
        help="LoRA é‡ã¿å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"
    )
    parser.add_argument(
        "--epochs",
        type=int,
        default=50,
        help="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒãƒƒã‚¯æ•°"
    )
    parser.add_argument(
        "--batch_size",
        type=int,
        default=1,
        help="ãƒãƒƒãƒã‚µã‚¤ã‚º (T4: 1-4æ¨å¥¨, A100: 4-8å¯èƒ½)"
    )
    parser.add_argument(
        "--learning_rate",
        type=float,
        default=1e-4,
        help="å­¦ç¿’ç‡"
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=32,
        help="DoRA ãƒ©ãƒ³ã‚¯ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 32, ãƒ¦ãƒ¼ã‚¶ãƒ¼å®Ÿæ¸¬æœ€é©å€¤ 32-64)"
    )
    parser.add_argument(
        "--lora_alpha",
        type=float,
        default=32,
        help="LoRA ã‚¢ãƒ«ãƒ•ã‚¡ (ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°)"
    )
    parser.add_argument(
        "--use_qlora",
        action="store_true",
        help="QLoRA ã‚’æœ‰åŠ¹åŒ– (direction: float16 é‡å­åŒ–, ãƒ¡ãƒ¢ãƒªå‰Šæ¸›)"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="è¨ˆç®—ãƒ‡ãƒã‚¤ã‚¹ (cuda, cpu)"
    )
    
    args = parser.parse_args()
    
    # ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (pip install å¾Œ)
    if not IMPORTS_SUCCESS:
        print("âš ï¸  Attempting to install required packages...")
        os.system("pip install -q diffusers transformers pillow torch tqdm safetensors")
    
    # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼å®Ÿè¡Œ
    trainer = LoRATrainer(
        output_dir=args.output_dir,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        device=args.device,
        use_qlora=args.use_qlora
    )
    
    training_log = trainer.train(
        data_dir=args.data_dir,
        num_epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate
    )
    
    print("\n" + "="*60)
    print("ğŸ‰ LoRA Training Complete!")
    print("="*60)
    print(f"ğŸ“ Output: {trainer.output_dir}")
    print(f"ğŸ“Š Final Loss: {training_log['history'][-1]['loss']:.6f}")
    print("="*60)


if __name__ == "__main__":
    main()
