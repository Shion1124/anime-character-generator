#!/usr/bin/env python3
"""
LLM ãƒ™ãƒ¼ã‚¹ã®å …ç‰¢ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³

Gao et al. (2306.13103) ã«ã‚ˆã‚‹è„†å¼±æ€§ç ”ç©¶ã«åŸºã¥ãã€ã‚¿ã‚¤ãƒãƒ»ã‚°ãƒªãƒ•æ”»æ’ƒãªã©ã®
æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¤ã‚ºã«å¯¾ã™ã‚‹è€æ€§ã‚’å¼·åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã‚’å®Ÿè£…ã€‚

èƒŒæ™¯ï¼šGao et al. ã¯ã€Text-to-Image ãƒ¢ãƒ‡ãƒ«ãŒã€ŒA photo of an astronautã€ã‚’
ã€ŒA photo of an astornautã€ï¼ˆã‚¿ã‚¤ãƒï¼‰ã«å¤‰ãˆã‚‹ã ã‘ã§ç”ŸæˆçµæœãŒåŠ‡çš„ã«å¤‰ã‚ã‚‹
ã“ã¨ã‚’å®Ÿé¨“ã§è¨¼æ˜ã—ãŸã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€è¤‡æ•°ã®é¡ä¼¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§
ã“ã®è„†å¼±æ€§ã‚’è»½æ¸›ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚

ä½¿ç”¨ä¾‹:
    generator = RobustPromptGenerator()
    result = generator.generate_prompt("happy", "formal dress")
    print(result["positive_prompt"])
"""

import google.generativeai as genai
import json
from typing import Dict, List, Optional
from pathlib import Path
import os
from dotenv import load_dotenv


class RobustPromptGenerator:
    """
    LLM ãƒ™ãƒ¼ã‚¹ã®å …ç‰¢ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    
    ç‰¹å¾´:
    - Google Gemini API ã‚’ä½¿ç”¨ã—ãŸå¤šå±¤æ§‹é€ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿæ§‹ï¼ˆAPI ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
    - ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ã
    - ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
    """
    
    def __init__(self, cache_dir: str = "./prompt_cache"):
        """
        åˆæœŸåŒ–
        
        Args:
            cache_dir: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # Gemini API åˆæœŸåŒ–
        load_dotenv()
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY ç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-2.0-flash')
        
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.cache = {}
        self._load_cache()
        
        print("âœ… RobustPromptGenerator initialized with Gemini API")
    
    def _load_cache(self):
        """å‰å›ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ­ãƒ¼ãƒ‰"""
        cache_file = self.cache_dir / "prompts.json"
        if cache_file.exists():
            try:
                with open(cache_file, "r", encoding="utf-8") as f:
                    self.cache = json.load(f)
                print(f"âœ… Loaded {len(self.cache)} cached prompts")
            except:
                print("âš ï¸  Failed to load cache, starting fresh")
                self.cache = {}
    
    def _save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        cache_file = self.cache_dir / "prompts.json"
        with open(cache_file, "w", encoding="utf-8") as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
    
    def generate_prompt(
        self,
        emotion: str,
        style: str,
        quality_level: str = "masterpiece",
        additional_context: Optional[str] = None
    ) -> Dict:
        """
        ãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        
        Args:
            emotion: æ„Ÿæƒ…ï¼ˆ"happy", "angry", "sad" ãªã©ï¼‰
            style: ã‚¹ã‚¿ã‚¤ãƒ«ï¼ˆ"formal", "casual" ãªã©ï¼‰
            quality_level: å“è³ªãƒ¬ãƒ™ãƒ«
            additional_context: è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            {
                "positive_prompt": str,
                "negative_prompt": str,
                "confidence": float,
                "metadata": dict,
                "layers": dict
            }
        """
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = f"{emotion}_{style}_{quality_level}"
        if cache_key in self.cache:
            print(f"ğŸ“¦ Using cached prompt for: {cache_key}")
            return self.cache[cache_key]
        
        print(f"ğŸ¤– Generating prompt: {emotion} + {style}...")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        system_prompt = """
ã‚ãªãŸã¯ Stable Diffusion v1.5 å‘ã‘ã®é«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚

ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ãã ã•ã„ï¼š
1. ãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹é€ ã§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
2. è¤‡æ•°ã®åŒç¾©è¡¨ç¾ã‚’æä¾›ï¼ˆæ‘‚å‹•è€æ€§å‘ä¸Šï¼‰
3. æ„Ÿæƒ…ã‚¿ã‚°ã¯æœ€ä½3ã¤ã€ã§ãã‚Œã°5ã¤æä¾›
4. ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®šã¯å…·ä½“çš„ã§æ›–æ˜§ã•æœ€å°åŒ–
5. è² ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¿…é ˆï¼ˆä½•ã‚’é¿ã‘ã‚‹ã‹æ˜ç¢ºã«ï¼‰
6. ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0-1.0ï¼‰ã‚’è¿”ã™

å¿œç­”ã¯ JSON å½¢å¼ã§ã€ä»¥ä¸‹ã®æ§‹é€ :
{
  "core": "åŸºæœ¬ãƒ™ãƒ¼ã‚¹ï¼ˆå¤‰æ›´ã«å¼·ã„ï¼‰",
  "emotion_tags": ["tag1", "tag2", "tag3", ...],
  "style_descriptors": ["style1", "style2", "style3", ...],
  "quality_modifiers": ["quality1", "quality2"],
  "negative_prompt": ["avoid1", "avoid2", ...],
  "confidence": 0.0-1.0,
  "reasoning": "ç”Ÿæˆç†ç”±ã®èª¬æ˜"
}
"""
        
        user_message = f"""{system_prompt}

Stable Diffusion v1.5 å‘ã‘ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ

æ„Ÿæƒ…: {emotion}
ã‚¹ã‚¿ã‚¤ãƒ«: {style}
å“è³ª: {quality_level}
{f'è¿½åŠ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {additional_context}' if additional_context else ''}

ä¸Šè¨˜ã«åŸºã¥ã„ã¦ã€å¤šå±¤æ§‹é€ ã®å …ç‰¢ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ JSON å½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            response = self.model.generate_content(
                contents=user_message,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=400,
                    temperature=0.7
                )
            )
            
            # JSON å¿œç­”ã‚’ãƒ‘ãƒ¼ã‚¹
            response_text = response.text
            
            # JSON ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            response_json = json.loads(response_text)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆ
            positive_parts = [
                response_json.get("core", "1girl, anime character"),
                ", ".join(response_json.get("emotion_tags", [])),
                ", ".join(response_json.get("style_descriptors", [])),
                ", ".join(response_json.get("quality_modifiers", []))
            ]
            positive_prompt = ", ".join([p for p in positive_parts if p])
            
            negative_prompt = ", ".join(
                response_json.get("negative_prompt", ["low quality"])
            )
            
            result = {
                "positive_prompt": positive_prompt,
                "negative_prompt": negative_prompt,
                "confidence": response_json.get("confidence", 0.8),
                "layers": {
                    "core": response_json.get("core"),
                    "emotion_tags": response_json.get("emotion_tags", []),
                    "style_descriptors": response_json.get("style_descriptors", []),
                    "quality_modifiers": response_json.get("quality_modifiers", [])
                },
                "metadata": {
                    "emotion": emotion,
                    "style": style,
                    "quality_level": quality_level,
                    "reasoning": response_json.get("reasoning", "")
                }
            }
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.cache[cache_key] = result
            self._save_cache()
            
            print(f"âœ… Generated prompt (confidence: {result['confidence']:.2f})")
            
            return result
        
        except json.JSONDecodeError as e:
            print(f"âš ï¸  Failed to parse JSON response: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return self._fallback_prompt(emotion, style, quality_level)
        
        except Exception as e:
            print(f"âŒ API Error: {e}")
            return self._fallback_prompt(emotion, style, quality_level)
    
    def validate_prompt(self, prompt: str) -> Dict:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªã®æ¤œè¨¼
        
        Gao et al. (2306.13103) ãŒç¤ºã—ãŸè„†å¼±æ€§ï¼ˆã‚¿ã‚¤ãƒãƒ»ã‚°ãƒªãƒ•æ”»æ’ƒã¸ã®æ•æ„Ÿæ€§ï¼‰
        ã‚’è€ƒæ…®ã—ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†—é•·æ€§ã¨å …ç‰¢æ€§ã‚’ã‚¹ã‚³ã‚¢åŒ–ã€‚
        
        Args:
            prompt: æ¤œè¨¼å¯¾è±¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        
        Returns:
            {
                "overall_score": 0-10,
                "ambiguity": 0-10,
                "consistency": 0-10,
                "robustness": 0-10,
                "recommendations": [...]
            }
        """
        
        print(f"ğŸ” Validating prompt...")
        
        validation_prompt = f"""
Stable Diffusion ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}

ä»¥ä¸‹ã®è¦³ç‚¹ã§ JSON å½¢å¼ã§è©•ä¾¡çµæœã‚’è¿”ã—ã¦ãã ã•ã„:
{{
  "overall_score": 0-10,
  "ambiguity_score": 0-10,
  "consistency_score": 0-10,
  "robustness_score": 0-10,
  "issues": ["issue1", "issue2", ...],
  "recommendations": ["recommendation1", ...],
  "summary": "è©•ä¾¡ã‚µãƒãƒªãƒ¼"
}}

ã‚¹ã‚³ã‚¢ã¯é«˜ã„ã»ã©è‰¯ã„å“è³ªã‚’ç¤ºã—ã¾ã™ã€‚
"""
        
        try:
            response = self.model.generate_content(
                contents=validation_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=300,
                    temperature=0.5
                )
            )
            
            response_text = response.text
            
            # JSON ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            return json.loads(response_text)
        except:
            return {"error": "Validation failed"}
    
    def compare_prompts(self, prompt1: str, prompt2: str) -> Dict:
        """
        2ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¯”è¼ƒ
        
        Args:
            prompt1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1
            prompt2: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2
        
        Returns:
            æ¯”è¼ƒçµæœ
        """
        
        print(f"âš–ï¸  Comparing prompts...")
        
        compare_prompt = f"""
2ã¤ã® Stable Diffusion ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1: {prompt1}
ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2: {prompt2}

JSON å½¢å¼ã§æ¯”è¼ƒçµæœã‚’è¿”ã—ã¦ãã ã•ã„:
{{
  "better": 1 or 2,
  "reason": "ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã€ç†ç”±",
  "prompt1_score": 0-10,
  "prompt2_score": 0-10,
  "robustness_difference": "æ‘‚å‹•è€æ€§ã®å·®",
  "recommendations": [...]
}}
"""
        
        try:
            response = self.model.generate_content(
                contents=compare_prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=300,
                    temperature=0.5
                )
            )
            
            response_text = response.text
            
            # JSON ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            if "```json" in response_text:
                json_start = response_text.find("```json") + 7
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            elif "```" in response_text:
                json_start = response_text.find("```") + 3
                json_end = response_text.find("```", json_start)
                response_text = response_text[json_start:json_end].strip()
            
            return json.loads(response_text)
        except:
            return {"error": "Comparison failed"}
    
    def _fallback_prompt(self, emotion: str, style: str, quality: str) -> Dict:
        """
        API å¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        print("âš ï¸  Using fallback prompt")
        
        emotion_map = {
            "happy": ["happy", "cheerful", "smiling", "bright"],
            "angry": ["angry", "fierce", "intense", "determined"],
            "sad": ["sad", "melancholic", "crying", "sorrowful"],
            "neutral": ["neutral", "calm", "peaceful", "serene"],
        }
        
        style_map = {
            "casual": ["casual clothes", "relaxed pose", "everyday outfit"],
            "formal": ["formal dress", "elegant", "sophisticated"],
            "magical": ["magical", "fantasy", "mystical", "enchanting"],
            "anime": ["anime style", "detailed", "expressive"],
        }
        
        emotion_tags = emotion_map.get(emotion, [emotion])[:3]
        style_desc = style_map.get(style, [style])[:3]
        
        positive = f"1girl, anime character, {', '.join(emotion_tags)}, {', '.join(style_desc)}, {quality}"
        negative = "low quality, blurry, deformed, ugly, bad anatomy"
        
        return {
            "positive_prompt": positive,
            "negative_prompt": negative,
            "confidence": 0.5,
            "layers": {
                "core": "1girl, anime character",
                "emotion_tags": emotion_tags,
                "style_descriptors": style_desc,
                "quality_modifiers": [quality]
            },
            "metadata": {
                "emotion": emotion,
                "style": style,
                "quality_level": quality,
                "reasoning": "Fallback prompt"
            }
        }


def main():
    """ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    
    print("ğŸš€ Starting RobustPromptGenerator demo\n")
    
    try:
        generator = RobustPromptGenerator()
        
        # ãƒ†ã‚¹ãƒˆ 1: happy + formal
        print("\n" + "="*60)
        print("ãƒ†ã‚¹ãƒˆ 1: happy + formal")
        print("="*60)
        result1 = generator.generate_prompt("happy", "formal dress")
        print(f"\nâœ¨ Positive Prompt:\n{result1['positive_prompt']}\n")
        print(f"âŒ Negative Prompt:\n{result1['negative_prompt']}\n")
        print(f"ğŸ“Š Confidence: {result1['confidence']:.2f}\n")
        
        # ãƒ†ã‚¹ãƒˆ 2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªï¼ˆåŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒƒãƒˆã§å†åº¦å®Ÿè¡Œï¼‰
        print("\n" + "="*60)
        print("ãƒ†ã‚¹ãƒˆ 2: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªï¼ˆåŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒƒãƒˆã§å†åº¦å®Ÿè¡Œï¼‰")
        print("="*60)
        result2 = generator.generate_prompt("happy", "formal dress")
        print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸ\n")
        
        # ãƒ†ã‚¹ãƒˆ 3: åˆ¥ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒƒãƒˆ
        print("\n" + "="*60)
        print("ãƒ†ã‚¹ãƒˆ 3: sad + casual")
        print("="*60)
        result3 = generator.generate_prompt("sad", "casual")
        print(f"\nâœ¨ Positive Prompt:\n{result3['positive_prompt']}\n")
        
        # ãƒ†ã‚¹ãƒˆ 4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒƒãƒˆæ¤œè¨¼
        print("\n" + "="*60)
        print("ãƒ†ã‚¹ãƒˆ 4: ãƒ—ãƒ­ãƒ³ãƒ—ãƒƒãƒˆæ¤œè¨¼")
        print("="*60)
        validation = generator.validate_prompt(result1["positive_prompt"])
        if "error" not in validation:
            print(f"ğŸ“Š Validation Result:\n{json.dumps(validation, ensure_ascii=False, indent=2)}\n")
        
        print("âœ… ãƒ‡ãƒ¢å®Ÿè¡Œå®Œäº†ï¼")
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
