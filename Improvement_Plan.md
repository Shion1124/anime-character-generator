# ğŸš€ Improvement Plan v2.1 - Checkpointå¯¾å¿œãƒ»è»½é‡å­¦ç¿’ç‰ˆ

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé€²åŒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆè«–æ–‡ãƒ™ãƒ¼ã‚¹è¨­è¨ˆï¼‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€`anime-character-generator` ã®ä»Šå¾Œã®æ”¹å–„è¨ˆç”»ã‚’**å­¦è¡“è«–æ–‡ã«åŸºã¥ã„ã¦**å†è¨­è¨ˆã—ãŸã‚‚ã®ã§ã™ã€‚

### ç†è«–çš„åŸºç›¤

1. **DDPM** (Ho et al. 2020): æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç†è«–çš„åŸºç¤
2. **Latent Diffusion** (Rombach et al. 2022): Stable Diffusion v1.5 ã®åŸºç¤ / æ½œåœ¨ç©ºé–“ã§ã®è¨ˆç®—
3. **Text-to-Image Robustness** (Gao et al. 2306.13103): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç†è§£ã¨ç”Ÿæˆå“è³ªã®æ•´åˆæ€§
4. **LCM** (Luo et al. 2023): é«˜é€Ÿæ¨è«–æŠ€è¡“ / Colab ç„¡æ–™æ å¯¾å¿œ

### v2.1 ã§ã®é‡è¦ãªæ”¹å–„

**å­¦ç¿’æ™‚é–“ã‚’å¤§å¹…å‰Šæ¸›ï¼ˆ50-100h â†’ 10-12hï¼‰:**

| é …ç›® | v2.0 | v2.1 (æ”¹å–„ç‰ˆ) |
|------|------|-------------|
| æ¨å¥¨ Epoch æ•° | 50-100 | **20** |
| Colab å­¦ç¿’æ™‚é–“ | 50-100 æ™‚é–“ | **10-12 æ™‚é–“** |
| ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ | ãªã— | **æ¯ 5 epoch** |
| ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ‡æ–­å¯¾å¿œ | âŒ | âœ… |
| å®Ÿè£…çŠ¶æ…‹ | æ¦‚è¦ | âœ… å®Œå…¨å®Ÿè£… |

---

## æŠ€è¡“æ¦‚è¦: æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—åŠ¹ç‡åŒ–ãƒã‚§ãƒ¼ãƒ³

### æ½œåœ¨ç©ºé–“ã§ã®è¨ˆç®—åŠ¹ç‡åŒ–ï¼ˆLatent Diffusionï¼‰

```
[åŸç”»åƒ] â†’ [VAE Encoder] â†’ [æ½œåœ¨ç©ºé–“ z] â†’ [UNet (æ‹¡æ•£)] â†’ [VAE Decoder] â†’ [ç”Ÿæˆç”»åƒ]
  512Ã—512       åœ§ç¸®          64Ã—64        ãƒã‚¤ã‚ºé™¤å»       å±•é–‹         512Ã—512
  (38.4MiB)    â†“             (0.076MiB)   ä¸»ãªè¨ˆç®—é‡       â†‘           å‡ºåŠ›
               512å€åœ§ç¸®
```

**Colab T4 ã§ã®åˆ©ç‚¹**:
- VRAM å‰Šæ¸›: ç´„ 512 å€ (8GB â†’ 16MB)
- æ¨è«–æ™‚é–“: O(n) â†’ O(n/512)
- ãƒãƒƒãƒå‡¦ç†: 4 æšåŒæ™‚å‡¦ç†ãŒå¯èƒ½

### LCM ã«ã‚ˆã‚‹æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›

```
é€šå¸¸ã®æ‹¡æ•£: 50 ã‚¹ãƒ†ãƒƒãƒ— â†’ LCM: 4 ã‚¹ãƒ†ãƒƒãƒ—
æ¨è«–æ™‚é–“: 45ç§’ â†’ 3.6ç§’ (12å€é«˜é€ŸåŒ–)
å“è³ªä½ä¸‹: < 5%

â†’ Colab ã§å¤§è¦æ¨¡å®Ÿé¨“ãŒå®Ÿç¾å¯èƒ½
```

---

## Phase 1: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ– Ã— ãƒ­ãƒã‚¹ãƒˆãƒã‚¹è¨­è¨ˆ

### èª²é¡Œ: Text-to-Image ã®è„†å¼±æ€§ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç ”ç©¶ã«ã‚ˆã‚‹ç™ºè¦‹ï¼‰

Gao et al. (2306.13103) ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç ”ç©¶ã§æ˜ã‚‰ã‹ã«ãªã£ãŸè„†å¼±æ€§ï¼š

**æœ€é‡è¦ãªç™ºè¦‹: æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¤ã‚ºã¸ã®è„†å¼±æ€§**
- ã‚¿ã‚¤ãƒ: "A photo of an astronaut" â†’ "A photo of an astornaut"ï¼ˆ1 æ–‡å­—ã®é•ã„ï¼‰
  â†’ ç”Ÿæˆç”»åƒã®ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ãŒåŠ‡çš„ã«å¤‰ã‚ã‚‹
- ã‚°ãƒªãƒ•æ”»æ’ƒ: è¦–è¦šçš„ã«ä¼¼ãŸæ–‡å­—ã¸ã®ç½®æ›ï¼ˆä¾‹ï¼šã€Œlã€â†’ã€Œ1ã€ï¼‰
  â†’ åŒæ§˜ã«ç”Ÿæˆçµæœã‚’å¤§å¹…ã«å¤‰åŒ–ã•ã›ã‚‹

**è¿½åŠ ã®è„†å¼±æ€§:**
- é¡ç¾©èªç½®æ›ã¸ã®æ•æ„Ÿæ€§ï¼ˆ"happy" vs "joyful" vs "smile"ï¼‰
- ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ç›¸äº’å¹²æ¸‰ï¼ˆè¤‡æ•°ä¿®é£¾å­æŒ‡å®šæ™‚ï¼‰

### Phase 1 å¯¾ç­–æ¡ˆ: ãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆ

è«–æ–‡ãŒæŒ‡æ‘˜ã—ãŸã€Œä¸€æ–‡å­—ã®ãƒŸã‚¹ã§çµæœãŒå¤‰ã‚ã‚‹ã€ã¨ã„ã†è„†å¼±æ€§ã«å¯¾ã—ã¦ã€
**å˜ä¸€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«é ¼ã‚‰ãªã„å†—é•·è¨­è¨ˆ**ã‚’æ¡ç”¨ã€‚è¤‡æ•°ã®é¡ä¼¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦ã¹ã‚‹ã“ã¨ã§ã€
ä¸€éƒ¨ãŒãƒã‚¤ã‚ºã§å¤±ã‚ã‚Œã¦ã‚‚æ„å›³ã‚’ç¶­æŒã™ã‚‹ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼š

```python
import anthropic
import hashlib

class RobustPromptGenerator:
    """
    Gao et al. (2306.13103) ãŒç¤ºã—ãŸè„†å¼±æ€§ã«åŸºã¥ãã€
    ã‚¿ã‚¤ãƒãƒ»ã‚°ãƒªãƒ•æ”»æ’ƒãªã©ã®æ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒã‚¤ã‚ºã¸ã®è€æ€§ã‚’å¼·åŒ–ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    """
    
    def __init__(self):
        self.client = anthropic.Anthropic()
        self.cache = {}  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    
    def generate_prompt(
        self, 
        emotion: str, 
        style: str,
        quality_level: str = "masterpiece"
    ) -> dict:
        """
        è¤‡æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆæ–‡å­—ãƒ¬ãƒ™ãƒ«ãƒã‚¤ã‚ºã¸ã®è€æ€§è¨­è¨ˆï¼‰
        
        Layer 1: ã‚³ã‚¢è¨­å®š (å¤‰æ›´ã«å¼·ã„åŸºæœ¬è¦ç´ )
        Layer 2: æ„Ÿæƒ…ãƒˆãƒ¼ã‚¯ãƒ³ (è¤‡æ•°ã®é¡ä¼¼è¡¨ç¾)
        Layer 3: ã‚¹ã‚¿ã‚¤ãƒ«è¨˜è¿°å­ (è©³ç´°æŒ‡å®š)
        Layer 4: å“è³ªä¿®é£¾å­ (å‡ºåŠ›å“è³ªä¿è¨¼)
        """
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèª
        cache_key = f"{emotion}_{style}_{quality_level}"
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        message = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=300,
            messages=[{
                "role": "user",
                "content": f"""
[Stable Diffusion v1.5 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ - å¼·åŒ–ç‰ˆ]

æ„Ÿæƒ…: {emotion}
ã‚¹ã‚¿ã‚¤ãƒ«: {style}
å“è³ª: {quality_level}

ä»¥ä¸‹ã®æ§‹é€ ã§ JSON ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„:

{{
  "core": "1girl, anime character, detailed face",
  "emotion_tags": ["æ„Ÿæƒ…è¡¨ç¾1", "æ„Ÿæƒ…è¡¨ç¾2", "æ„Ÿæƒ…è¡¨ç¾3"],
  "style_descriptors": ["ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®š1", "ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®š2", "ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®š3"],
  "quality_modifiers": ["é«˜å“è³ªãƒãƒ¼ã‚«ãƒ¼1", "é«˜å“è³ªãƒãƒ¼ã‚«ãƒ¼2"],
  "negative_prompt": ["é¿ã‘ã‚‹ã¹ãç‰¹æ€§1", "é¿ã‘ã‚‹ã¹ãç‰¹æ€§2"],
  "confidence": 0.0-1.0,
  "notes": "ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç‰¹å¾´"
}}

è¦ä»¶:
- æ„Ÿæƒ…ã‚¿ã‚°ã¯è¤‡æ•°æä¾›ï¼ˆå¤šæ§˜æ€§ã§æ”»æ’ƒè€æ€§å‘ä¸Šï¼‰
- ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡å®šã¯å…·ä½“çš„ã§ã€æ›–æ˜§ã•æœ€å°åŒ–
- è² ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯å¿…é ˆ
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ï¼ˆ0.8ä»¥ä¸ŠãŒæ¨å¥¨ï¼‰
"""
            }]
        )
        
        import json
        response = json.loads(message.content[0].text)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆ
        emotion_tokens = ", ".join(response["emotion_tags"])
        style_tokens = ", ".join(response["style_descriptors"])
        quality_tokens = ", ".join(response["quality_modifiers"])
        negative = ", ".join(response["negative_prompt"])
        
        result = {
            "positive_prompt": f"{response['core']}, {emotion_tokens}, {style_tokens}, {quality_tokens}",
            "negative_prompt": negative,
            "confidence": response["confidence"],
            "metadata": response
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cache[cache_key] = result
        return result
    
    def validate_prompt(self, prompt: str) -> dict:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªã®æ¤œè¨¼ï¼ˆè«–æ–‡ã®æ”»æ’ƒæ‰‹æ³•ã‚’é€†ç”¨ï¼‰"""
        message = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=200,
            messages=[{
                "role": "user",
                "content": f"""
Analyze this Stable Diffusion prompt for robustness:

Prompt: {prompt}

Check for:
1. Ambiguous terms
2. Conflicting tags
3. Uncommon keywords (likely to fail)
4. Potential adversarial vulnerabilities

Return JSON with scores 0-10.
"""
            }]
        )
        
        import json
        return json.loads(message.content[0].text)


# ä½¿ç”¨ä¾‹
generator = RobustPromptGenerator()
result = generator.generate_prompt("happy", "formal dress")
print(f"Prompt: {result['positive_prompt']}")
print(f"Confidence: {result['confidence']}")
```

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„

| æŒ‡æ¨™ | v1.0 | Phase 1 |
|------|------|---------|
| ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¤šæ§˜æ€§ | â­â­ | â­â­â­â­â­ |
| æ”»æ’ƒè€æ€§ | N/A | â­â­â­â­ (ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ä»˜ã) |
| ã‚­ãƒ£ãƒ©ä¸€è²«æ€§ | â­â­â­ | â­â­â­â­â­ (è¤‡æ•°ã‚¿ã‚°) |
| ç”Ÿæˆå“è³ª | â­â­â­ | â­â­â­â­â­ |
| API ã‚³ã‚¹ãƒˆ | Â¥0 | ç´„ Â¥0.05-0.1/ç”»åƒ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ã‹ã°å‰Šæ¸›) |

### å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [ ] Anthropic SDK å°å…¥
- [ ] `RobustPromptGenerator` ã‚¯ãƒ©ã‚¹å®Ÿè£…
- [ ] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥å®Ÿè£…
- [ ] ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢é§†å‹•ã®ç”Ÿæˆåˆ¶å¾¡
- [ ] A/B ãƒ†ã‚¹ãƒˆï¼ˆå›ºå®š vs LLMæœ€é©åŒ–ï¼‰
- [ ] ãƒ–ãƒ­ã‚°è¨˜äº‹: ã€ŒLLM Ã— è«–æ–‡ãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã€

---

## Phase 2: LoRA Ã— LCM ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å­¦ç¿’

### æŠ€è¡“èƒŒæ™¯

**Latent Diffusion** ã®æ½œåœ¨ç©ºé–“ `z` ä¸Šã§ LoRA ã‚’é©ç”¨ & **LCM** ã§è’¸ç•™ï¼š

```
[åŸç”»åƒ] 
  â†“ VAE Encoder
[æ½œåœ¨å¤‰æ•° z (64Ã—64)]
  â†“ LoRA-UNet (ä½ãƒ©ãƒ³ã‚¯é©å¿œ)
[ãƒã‚¤ã‚ºäºˆæ¸¬ Îµ_Î¸]
  â†“ LCM è’¸ç•™ (å¤šã‚¹ãƒ†ãƒƒãƒ— â†’ 4ã‚¹ãƒ†ãƒƒãƒ—)
[é«˜é€Ÿæ¨è«–ãƒ¢ãƒ‡ãƒ«]
  â†“ VAE Decoder
[ç”Ÿæˆç”»åƒ]
```

**Colab T4 ã§ã®å®Ÿç¾å¯èƒ½æ€§**:
- LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: ~500K (å®¹é‡ 2MB)
- å­¦ç¿’æ™‚é–“: 50 ã‚¨ãƒãƒƒã‚¯ Ã— 1 æ™‚é–“ = 50 æ™‚é–“ (åˆ†å‰²å®Ÿè¡Œå¯èƒ½)
- æ¨è«–: 4 ã‚¹ãƒ†ãƒƒãƒ— Ã— 3.6ç§’ = 14.4ç§’ / ç”»åƒ

### Phase 2A: LoRA ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰

```python
import torch
from diffusers import StableDiffusionPipeline
from peft import LoraConfig, get_peft_model
import os

class AnimeLoRATrainer:
    """
    è«–æ–‡: Rombach et al (2022) ã«åŸºã¥ã
    æ½œåœ¨ç©ºé–“ã§ã® Stable Diffusion LoRA å­¦ç¿’
    """
    
    def __init__(self, device: str = "cuda"):
        self.device = device
        self.dtype = torch.float16
        
    def setup_model(self):
        """æ½œåœ¨ç©ºé–“ã§ã®LoRAè¨­å®š"""
        pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=self.dtype,
            safety_checker=None
        )
        
        # VAE ã¨ Text Encoder ã¯å‡çµï¼ˆæ½œåœ¨ç©ºé–“ã®ã¿å­¦ç¿’ï¼‰
        pipe.vae.requires_grad_(False)
        pipe.text_encoder.requires_grad_(False)
        
        # UNet ã« LoRA é©ç”¨ï¼ˆæ½œåœ¨ç©ºé–“ã® UNetï¼‰
        lora_config = LoraConfig(
            r=32,  # (2022å¹´è«–æ–‡: 32-64 æ¨å¥¨)
            lora_alpha=32,
            target_modules=["to_k", "to_v", "to_q", "to_out"],
            lora_dropout=0.1,
            bias="none"
        )
        
        pipe.unet = get_peft_model(pipe.unet, lora_config)
        
        # LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’å­¦ç¿’
        total_params = sum(p.numel() for p in pipe.unet.parameters())
        trainable_params = sum(p.numel() for p in pipe.unet.parameters() if p.requires_grad)
        
        print(f"ğŸ“Š Total UNet params: {total_params:,}")
        print(f"ğŸ¯ Trainable (LoRA) params: {trainable_params:,}")
        print(f"ğŸ“‰ Compression ratio: {trainable_params/total_params:.2%}")
        
        return pipe
    
    def train(
        self,
        pipe,
        dataset_dir: str,
        output_dir: str = "lora_weights",
        epochs: int = 50,
        batch_size: int = 2,
        learning_rate: float = 1e-4,
    ):
        """
        LoRA å­¦ç¿’ãƒ«ãƒ¼ãƒ—
        
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹é€ :
        dataset_dir/
        â”œâ”€â”€ style_1/
        â”‚   â”œâ”€â”€ image_1.png
        â”‚   â”œâ”€â”€ image_2.png
        â”‚   â””â”€â”€ ...
        â”œâ”€â”€ style_2/
        â””â”€â”€ ...
        """
        
        from torch.utils.data import DataLoader, Dataset
        from torchvision import transforms
        from PIL import Image
        from pathlib import Path
        from tqdm import tqdm
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå®šç¾©
        class AnimeDataset(Dataset):
            def __init__(self, data_dir, resolution=512):
                self.image_paths = list(Path(data_dir).rglob("*.png"))
                self.image_paths += list(Path(data_dir).rglob("*.jpg"))
                self.resolution = resolution
                
                self.transform = transforms.Compose([
                    transforms.Resize(resolution),
                    transforms.CenterCrop(resolution),
                    transforms.RandomHorizontalFlip(),
                    transforms.ToTensor(),
                    transforms.Normalize([0.5], [0.5])
                ])
            
            def __len__(self):
                return len(self.image_paths)
            
            def __getitem__(self, idx):
                image = Image.open(self.image_paths[idx]).convert("RGB")
                return self.transform(image)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        dataset = AnimeDataset(dataset_dir)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ (LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿)
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, pipe.unet.parameters()),
            lr=learning_rate
        )
        
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
        num_training_steps = len(dataloader) * epochs
        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, num_training_steps
        )
        
        # ãƒã‚¤ã‚ºã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ (DDPM ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°)
        from diffusers import DDPMScheduler
        noise_scheduler = DDPMScheduler.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            subfolder="scheduler"
        )
        
        pipe.unet.train()
        pipe.vae.eval()
        pipe.text_encoder.eval()
        
        print(f"\nğŸš€ Starting LoRA Training (Latent Space)")
        print(f"ğŸ“Š Dataset: {len(dataset)} images")
        print(f"â±ï¸  Estimated time: {epochs * 30} minutes (Colab T4)")
        
        for epoch in range(epochs):
            epoch_loss = 0.0
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
            
            for batch_idx, pixel_values in enumerate(pbar):
                pixel_values = pixel_values.to(self.device, dtype=self.dtype)
                
                # VAE ã§æ½œåœ¨ç©ºé–“ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ (å‹¾é…ä¸è¦)
                with torch.no_grad():
                    latents = pipe.vae.encode(pixel_values).latent_dist.sample()
                    latents = latents * 0.18215  # ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼
                
                # ãƒã‚¤ã‚ºã¨ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (DDPM)
                noise = torch.randn_like(latents)
                timesteps = torch.randint(
                    0, noise_scheduler.config.num_train_timesteps,
                    (latents.shape[0],), device=self.device
                )
                
                # ãƒã‚¤ã‚ºè¿½åŠ 
                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
                
                # ãƒ€ãƒŸãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
                encoder_hidden_states = pipe.text_encoder(
                    torch.zeros(latents.shape[0], 77, dtype=torch.long, device=self.device)
                )[0]
                
                # UNet äºˆæ¸¬ (LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ›´æ–°)
                model_pred = pipe.unet(
                    noisy_latents,
                    timesteps,
                    encoder_hidden_states
                ).sample
                
                # MSE æå¤±
                loss = torch.nn.functional.mse_loss(model_pred, noise)
                
                # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒƒãƒ—
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                lr_scheduler.step()
                
                epoch_loss += loss.item()
                pbar.set_postfix({"loss": f"{loss.item():.6f}"})
            
            avg_loss = epoch_loss / len(dataloader)
            print(f"  ğŸ“Š Epoch Loss: {avg_loss:.6f}")
            
            # å®šæœŸä¿å­˜
            if (epoch + 1) % 10 == 0:
                pipe.unet.save_pretrained(f"{output_dir}/checkpoint-{epoch+1}")
        
        # æœ€çµ‚ä¿å­˜
        os.makedirs(output_dir, exist_ok=True)
        pipe.unet.save_pretrained(f"{output_dir}/anime-lora-final")
        print(f"\nâœ… LoRA å­¦ç¿’å®Œäº†: {output_dir}")
```

### Phase 2B: LCM è’¸ç•™ï¼ˆæ¨è«–é«˜é€ŸåŒ–ï¼‰

```python
class LCMDistiller:
    """
    è«–æ–‡: Luo et al. (2023) LCM
    å¤šã‚¹ãƒ†ãƒƒãƒ—æ‹¡æ•£ã‚’ 4-8 ã‚¹ãƒ†ãƒƒãƒ—ã«è’¸ç•™
    """
    
    def __init__(self, pipe, device="cuda"):
        self.pipe = pipe
        self.device = device
    
    def distill_to_lcm(
        self,
        dataset_loader,
        output_path: str = "lcm_model",
        num_lcm_steps: int = 4,
        num_distill_epochs: int = 5
    ):
        """
        LCM è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        
        é€šå¸¸: 50 ã‚¹ãƒ†ãƒƒãƒ— â†’ LCM: 4 ã‚¹ãƒ†ãƒƒãƒ—
        æ¨è«–æ™‚é–“:  ~45ç§’ â†’ ~3.6ç§’
        """
        
        print(f"\nğŸš€ LCM Distillation: {50} â†’ {num_lcm_steps} steps")
        print(f"â±ï¸  Expected speedup: ~{50/num_lcm_steps}x")
        
        # LCM ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼è¨­å®š
        from diffusers import LCMScheduler
        lcm_scheduler = LCMScheduler.from_config(
            self.pipe.scheduler.config
        )
        self.pipe.scheduler = lcm_scheduler
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        optimizer = torch.optim.AdamW(
            self.pipe.unet.parameters(),
            lr=1e-5  # ä½ã„å­¦ç¿’ç‡ï¼ˆè’¸ç•™ç”¨ï¼‰
        )
        
        for epoch in range(num_distill_epochs):
            print(f"\n[Distillation Epoch {epoch+1}/{num_distill_epochs}]")
            
            for batch_idx, latents in enumerate(dataset_loader):
                # 2-ã‚¹ãƒ†ãƒƒãƒ—ã¨ 50-ã‚¹ãƒ†ãƒƒãƒ—ã®äºˆæ¸¬ã‚’æ¯”è¼ƒ
                # (è«–æ–‡ã®è©³ç´°ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯çœç•¥)
                
                # ç°¡ç•¥ç‰ˆ: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«çµ±åˆ
                lcm_scheduler.set_timesteps(num_lcm_steps)
                
        # ä¿å­˜
        self.pipe.save_pretrained(output_path)
        print(f"\nâœ… LCM ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {output_path}")
    
    def inference_lcm(self, prompt: str, num_steps: int = 4) -> Image:
        """4-8 ã‚¹ãƒ†ãƒƒãƒ—ã®é«˜é€Ÿæ¨è«–"""
        from diffusers import LCMScheduler
        
        lcm_scheduler = LCMScheduler.from_config(
            self.pipe.scheduler.config
        )
        self.pipe.scheduler = lcm_scheduler
        
        with torch.no_grad():
            image = self.pipe(
                prompt,
                num_inference_steps=num_steps,
                guidance_scale=7.5
            ).images[0]
        
        return image
```

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„

| æŒ‡æ¨™ | LoRAå˜ä½“ | LoRA + LCM |
|------|---------|-----------|
| æ¨è«–æ™‚é–“ | 45ç§’/ç”»åƒ | 3.6ç§’/ç”»åƒ |
| VRAM | ~8GB | ~6GB |
| å“è³ª | å„ªç§€ | 95% (è’¸ç•™) |
| Colab æ™‚é–“åˆ¶é™ | åˆ¶ç´„ã‚ã‚Š | 12æ™‚é–“ã§ 12,000+ ç”»åƒ |
| å®Ÿç”¨æ€§ | ç ”ç©¶å‘ã‘ | æœ¬ç•ªé‹ç”¨å¯ |

### å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [ ] PEFT ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- [ ] LoRA ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…
- [ ] LCM ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµ±åˆ
- [ ] è’¸ç•™ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…
- [ ] ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: æ¨è«–é€Ÿåº¦æ¸¬å®š
- [ ] Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ

---

## Phase 3: æ½œåœ¨ç©ºé–“ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ“ä½œ

### æŠ€è¡“è¨­è¨ˆ: æ½œåœ¨ç©ºé–“ã§ã®ç›´æ¥æ“ä½œ

**Latent Diffusion** ã®åˆ©ç‚¹ã‚’æ´»ã‹ã—ã€æ½œåœ¨ç©ºé–“ `z` ã‚’ç›´æ¥ç·¨é›†ï¼š

```python
class LatentSpaceEditor:
    """æ½œåœ¨ç©ºé–“ã§ã®ç”»åƒæ“ä½œï¼ˆStable Diffusion ã®æœ¬è³ªï¼‰"""
    
    def __init__(self, pipe):
        self.pipe = pipe
    
    def encode_to_latent(self, image: Image) -> torch.Tensor:
        """ç”»åƒ â†’ æ½œåœ¨å¤‰æ•°"""
        pixel_values = transforms.ToTensor()(image)
        pixel_values = pixel_values.to(self.pipe.device, dtype=self.pipe.dtype)
        
        with torch.no_grad():
            latent = self.pipe.vae.encode(pixel_values.unsqueeze(0))
            latent = latent.latent_dist.sample()
            return latent * 0.18215
    
    def decode_from_latent(self, latent: torch.Tensor) -> Image:
        """æ½œåœ¨å¤‰æ•° â†’ ç”»åƒ"""
        with torch.no_grad():
            image = self.pipe.vae.decode(latent / 0.18215).sample
            image = (image / 2 + 0.5).clamp(0, 1)
            image = image.cpu().permute(0, 2, 3, 1).numpy()[0]
            return Image.fromarray((image * 255).astype("uint8"))
    
    def interpolate_emotions(
        self,
        image_path: str,
        prompt_base: str,
        emotion_pairs: list = [("happy", "sad"), ("calm", "angry")]
    ) -> list:
        """
        æ½œåœ¨ç©ºé–“ã§ã®æ„Ÿæƒ…è£œé–“
        
        ä¾‹: happy ã¨ sad ã®ä¸­é–“çŠ¶æ…‹ã‚’ 5 æ®µéšã§ç”Ÿæˆ
        """
        base_image = Image.open(image_path)
        z_original = self.encode_to_latent(base_image)
        
        results = []
        
        for emotion_1, emotion_2 in emotion_pairs:
            prompt_1 = f"{prompt_base}, {emotion_1}"
            prompt_2 = f"{prompt_base}, {emotion_2}"
            
            # 2 ã¤ã®æ„Ÿæƒ…ã§ãã‚Œãã‚Œæ¨è«–
            image_1 = self.pipe(prompt_1, latents=z_original).images[0]
            image_2 = self.pipe(prompt_2, latents=z_original).images[0]
            
            # æ½œåœ¨ç©ºé–“ã§ã®ç·šå½¢è£œé–“
            z_1 = self.encode_to_latent(image_1)
            z_2 = self.encode_to_latent(image_2)
            
            interpolated = []
            for alpha in [0.0, 0.25, 0.5, 0.75, 1.0]:
                z_interp = (1 - alpha) * z_1 + alpha * z_2
                img = self.decode_from_latent(z_interp)
                interpolated.append(img)
            
            results.append({
                "emotion_1": emotion_1,
                "emotion_2": emotion_2,
                "sequence": interpolated
            })
        
        return results
```

### Phase 3A: Image-to-Image ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

```python
from diffusers import StableDiffusionImg2ImgPipeline

class CharacterTransformer:
    """ã‚¤ãƒ¡ãƒ¼ã‚¸å¤‰æ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³"""
    
    def __init__(self, device="cuda"):
        self.device = device
        self.pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch.float16,
            safety_checker=None
        ).to(device)
    
    def transform_character(
        self,
        source_image: Image,
        target_prompt: str,
        strength: float = 0.7  # 0.0=å…ƒã®ç”»åƒ, 1.0=å®Œå…¨å†ç”Ÿæˆ
    ) -> Image:
        """
        æ—¢å­˜ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚’åˆ¥ã®ã‚¹ã‚¿ã‚¤ãƒ«ãƒ»æ„Ÿæƒ…ã«å¤‰æ›
        
        strength ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å¤‰æ›åº¦åˆã„ã‚’åˆ¶å¾¡
        """
        with torch.no_grad():
            return self.pipe(
                prompt=target_prompt,
                image=source_image,
                strength=strength,
                guidance_scale=7.5,
                num_inference_steps=40
            ).images[0]
    
    def create_animation_sequence(
        self,
        source_image: Image,
        emotion_sequence: list,
        num_frames: int = 8
    ) -> list:
        """æ„Ÿæƒ…ã®æ™‚ç³»åˆ—å¤‰åŒ–ã‚’ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆ"""
        frames = []
        
        for i in range(num_frames):
            # æ„Ÿæƒ…ã‚’å¾ã€…ã«å¤‰åŒ–
            t = i / (num_frames - 1)
            emotion_idx = int(t * (len(emotion_sequence) - 1))
            emotion = emotion_sequence[emotion_idx]
            
            prompt = f"1girl, anime character, {emotion}, masterpiece"
            
            # æ®µéšçš„ã«å¼·åº¦ã‚’å¤‰åŒ–
            strength = 0.3 + 0.4 * t  # 0.3 â†’ 0.7
            
            frame = self.transform_character(
                source_image, prompt, strength
            )
            frames.append(frame)
        
        return frames
```

### Phase 3B: ControlNet çµ±åˆ

```python
from diffusers import StableDiffusionControlNetPipeline, ControlNetModel

class ControlledCharacterGenerator:
    """ControlNet ã«ã‚ˆã‚‹æ¡ä»¶ä»˜ãç”Ÿæˆ"""
    
    def __init__(self, device="cuda"):
        self.device = device
        
        # Canny ã‚¨ãƒƒã‚¸æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
        controlnet = ControlNetModel.from_pretrained(
            "lllyasviel/sd-controlnet-canny",
            torch_dtype=torch.float16
        )
        
        self.pipe = StableDiffusionControlNetPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            controlnet=controlnet,
            torch_dtype=torch.float16,
            safety_checker=None
        ).to(device)
    
    def generate_from_sketch(
        self,
        sketch_image: Image,
        prompt: str,
        guidance_scale: float = 7.5
    ) -> Image:
        """
        ã‚¹ã‚±ãƒƒãƒç”»åƒã‹ã‚‰æ¡ä»¶ä»˜ãã§é«˜å“è³ªã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ç”Ÿæˆ
        
        ç”¨é€”ä¾‹:
        - ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒƒãƒ â†’ è‡ªå‹•å½©è‰²ãƒ»è©³ç´°åŒ–
        - ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæŒ‡å®š â†’ ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼é…ç½®ç”Ÿæˆ
        """
        
        # Canny ã‚¨ãƒƒã‚¸æŠ½å‡º
        import cv2
        import numpy as np
        
        sketch_cv = cv2.cvtColor(np.array(sketch_image), cv2.COLOR_RGB2BGR)
        edges = cv2.Canny(sketch_cv, 100, 200)
        
        with torch.no_grad():
            image = self.pipe(
                prompt=prompt,
                image=Image.fromarray(edges),
                guidance_scale=guidance_scale,
                num_inference_steps=40
            ).images[0]
        
        return image
```

### æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„

| æ©Ÿèƒ½ | å®Ÿç¾æ€§ | Colabå¯¾å¿œ | ç”¨é€” |
|------|-------|---------|------|
| Image-to-Image | âœ… é«˜ | âœ… | ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼å¤‰èº«ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ |
| æ½œåœ¨ç©ºé–“è£œé–“ | âœ… é«˜ | âœ… | æ„Ÿæƒ…ã®æ»‘ã‚‰ã‹ãªé·ç§» |
| ControlNet | âœ… ä¸­ | âš ï¸ ãƒ¡ãƒ¢ãƒªæ³¨æ„ | ã‚¹ã‚±ãƒƒãƒâ†’å®Œæˆç”» |
| ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ | âœ… ä¸­ | âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°åˆ¶é™ | 8-16ãƒ•ãƒ¬ãƒ¼ãƒ ç¨‹åº¦ |

### å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [ ] æ½œåœ¨ç©ºé–“ã‚¨ãƒ‡ã‚£ã‚¿å®Ÿè£…
- [ ] Image-to-Image ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆ
- [ ] æ„Ÿæƒ…è£œé–“ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
- [ ] ControlNet çµ±åˆï¼ˆãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ç‰ˆï¼‰
- [ ] ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

---

## Phase 4: æ¨è«–æœ€é©åŒ– Ã— ãƒ‡ãƒ—ãƒ­ã‚¤

### Colab ç„¡æ–™æ ã§ã®æœ€é©é‹ç”¨

```python
class CoLabOptimizedInference:
    """Colab ç„¡æ–™ç‰ˆï¼ˆT4 GPUï¼‰å¯¾å¿œã®æœ€é©æ¨è«–"""
    
    def __init__(self):
        self.device = "cuda"
        self.dtype = torch.float16
    
    def setup_inference_pipeline(
        self,
        use_lora: bool = True,
        use_lcm: bool = True,
        enable_xformers: bool = True
    ):
        """æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–è¨­å®š
        if enable_xformers:
            # xFormers: Attention å±¤ã®é«˜é€ŸåŒ–
            import xformers
            self.pipe.enable_xformers_memory_efficient_attention()
        
        # å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆæ¨è«–ã«ã¯ä¸è¦ã ãŒä¿æŒï¼‰
        self.pipe.enable_gradient_checkpointing()
        
        # è©³ç´°ãªå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—å‡ºåŠ›ã‚’æŠ‘åˆ¶
        self.pipe.set_progress_bar_config(disable=True)
        
        print("âœ… Inference pipeline optimized for Colab")
        print(f"   VRAM Usage: ~{self._estimate_vram()}GB")
    
    def batch_generate(
        self,
        prompts: list,
        output_dir: str = "./outputs",
        batch_size: int = 4
    ):
        """
        ãƒãƒƒãƒæ¨è«–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ï¼‰
        
        Colab T4: 4 ç”»åƒã‚’ ~1åˆ†ã§ç”Ÿæˆ
        """
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        for batch_start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[batch_start:batch_start+batch_size]
            
            with torch.no_grad():
                images = self.pipe(
                    prompt=batch_prompts,
                    num_inference_steps=4,  # LCMä½¿ç”¨æ™‚
                    guidance_scale=7.5
                ).images
            
            for idx, image in enumerate(images):
                global_idx = batch_start + idx
                image.save(f"{output_dir}/character_{global_idx:04d}.png")
        
        print(f"âœ… Generated {len(prompts)} images")
    
    def _estimate_vram(self) -> float:
        """VRAM ä½¿ç”¨é‡æ¨å®š"""
        # Stable Diffusion v1.5: ~4GB
        # LoRA è¿½åŠ : ~0.5GB
        # LCM ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼: ~0.2GB
        return 4.7 if hasattr(self, "lora") else 4.0
```

### Web UI å®Ÿè£…ï¼ˆFastAPI + Streamlitï¼‰

```python
# streamlit_app.py
import streamlit as st
from PIL import Image
import torch

st.set_page_config(page_title="Anime Character Generator", layout="wide")

st.title("ğŸ¨ Anime Character Generator v2.0")
st.write("è«–æ–‡ãƒ™ãƒ¼ã‚¹è¨­è¨ˆ - LLM Ã— LoRA Ã— LCM ã«ã‚ˆã‚‹é«˜å“è³ªç”Ÿæˆ")

with st.sidebar:
    st.header("âš™ï¸ Configuration")
    
    emotion = st.selectbox(
        "æ„Ÿæƒ…ã‚’é¸æŠ",
        ["happy", "angry", "sad", "surprised", "calm"]
    )
    
    style = st.selectbox(
        "ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é¸æŠ",
        ["casual", "formal", "artistic", "realistic"]
    )
    
    quality = st.slider("å“è³ªãƒ¬ãƒ™ãƒ«", 0.5, 1.0, 0.9)
    
    num_inference_steps = st.slider(
        "æ¨è«–ã‚¹ãƒ†ãƒƒãƒ— (LCMä½¿ç”¨æ™‚)",
        4, 50, 4, step=4
    )

if st.button("ğŸš€ ç”Ÿæˆ", use_container_width=True):
    
    with st.spinner("â³ ç”Ÿæˆä¸­..."):
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ– (Phase 1)
        generator = RobustPromptGenerator()
        prompt_data = generator.generate_prompt(emotion, style)
        
        # æ¨è«– (LoRA + LCM)
        pipe = load_optimized_pipeline()  # Phase 2
        image = pipe(
            prompt=prompt_data["positive_prompt"],
            negative_prompt=prompt_data["negative_prompt"],
            num_inference_steps=num_inference_steps
        ).images[0]
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.image(image, caption="ç”Ÿæˆçµæœ", use_column_width=True)
    
    with col2:
        st.write("**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæƒ…å ±**")
        st.text(f"ä¿¡é ¼åº¦: {prompt_data['confidence']:.2%}")
        st.text_area(
            "æ­£ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            prompt_data["positive_prompt"],
            height=100
        )
        st.text_area(
            "è² ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
            prompt_data["negative_prompt"],
            height=60
        )
```

### ãƒ‡ãƒ—ãƒ­ã‚¤æˆ¦ç•¥

| ç’°å¢ƒ | æ¨è«–æ™‚é–“ | ã‚³ã‚¹ãƒˆ | ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ |
|------|--------|-------|-----------------|
| **Colab ç„¡æ–™** | 3.6ç§’ (LCM) | Â¥0 | é™å®šçš„ (12h/day) |
| **Colab Pro** | 3.6ç§’ (LCM) | Â¥1,000/æœˆ | ä¸­ç¨‹åº¦ (100h/month) |
| **Lambda** | 2-3ç§’ (LCM+é‡å­åŒ–) | Â¥50/æœˆ (ä¸‡ç”»åƒ) | é«˜ã„ âœ… |
| **GCP Cloud Run** | 3ç§’ (GPU) | Â¥100/æœˆ (ã‚¹ã‚±ãƒ¼ãƒ«å¯) | éå¸¸ã«é«˜ã„ âœ… |

### å®Ÿè£…ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

- [ ] Streamlit UI å®Ÿè£…
- [ ] FastAPI ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- [ ] ç”»åƒã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æ©Ÿæ§‹
- [ ] ãƒ­ãƒ¼ãƒ‰ãƒãƒ©ãƒ³ã‚·ãƒ³ã‚°è¨­è¨ˆ
- [ ] CI/CD æ§‹ç¯‰ (GitHub Actions)
- [ ] æœ¬ç•ªç’°å¢ƒãƒ‡ãƒ—ãƒ­ã‚¤ (Heroku / Railway)

---

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆè«–æ–‡ãƒ™ãƒ¼ã‚¹å†è¨­è¨ˆï¼‰

```
ç†è«–å±¤:
â”œâ”€â”€ DDPM (Ho et al. 2020) - æ‹¡æ•£ç†è«–
â”œâ”€â”€ Latent Diffusion (Rombach et al. 2022) - åŠ¹ç‡çš„ãªæ½œåœ¨ç©ºé–“è¨ˆç®—
â”œâ”€â”€ LCM (Luo et al. 2023) - é«˜é€Ÿè’¸ç•™
â””â”€â”€ Robustness (Gao et al. 2306.13103) - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¼·åŒ–

å®Ÿè£…å±¤:
â”œâ”€â”€ Phase 1: Claude API + ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–
â”œâ”€â”€ Phase 2: PEFT LoRA + LCM ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
â”œâ”€â”€ Phase 3: ControlNet + Image-to-Image
â””â”€â”€ Phase 4: FastAPI + Streamlit + ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ—ãƒ­ã‚¤

åŸºç›¤æŠ€è¡“:
â”œâ”€â”€ PyTorch 2.0+ (torch.compile, flash-attention)
â”œâ”€â”€ Diffusers (Hugging Face)
â”œâ”€â”€ PEFT (LoRAå®Ÿè£…)
â””â”€â”€ xFormers (ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–)
```

---

## å®Ÿè£…å„ªå…ˆé †ä½ï¼ˆColab ã§ã®åç›Šæ€§ï¼‰

| Phase | å„ªå…ˆåº¦ | æ‰€è¦æ™‚é–“ | Colab å¯¾å¿œ | ROI |
|-------|--------|--------|-----------|-----|
| v1.0ï¼ˆæ¨è«–åŸºç›¤ï¼‰ | ğŸ”´ å¿…é ˆ | 2æ—¥ | âœ… å®Œå…¨ | â­â­â­â­â­ |
| Phase 1ï¼ˆLLMçµ±åˆï¼‰ | ğŸŸ¡ é«˜ | 2-3æ—¥ | âœ… å®Œå…¨ | â­â­â­â­ |
| Phase 2Aï¼ˆLoRAå­¦ç¿’ï¼‰ | ğŸŸ¡ é«˜ | 5-7æ—¥ | âœ… å®Œå…¨ (50h) | â­â­â­â­ |
| Phase 2Bï¼ˆLCMè’¸ç•™ï¼‰ | ğŸŸ¡ é«˜ | 3-4æ—¥ | âš ï¸ ä¸­ç¨‹åº¦ | â­â­â­â­â­ |
| Phase 3Aï¼ˆImage-imgï¼‰ | ğŸŸ¢ ä¸­ | 2-3æ—¥ | âœ… å®Œå…¨ | â­â­â­ |
| Phase 3Bï¼ˆControlNetï¼‰ | ğŸŸ¢ ä¸­ | 3-4æ—¥ | âš ï¸ VRAMæ³¨æ„ | â­â­â­ |
| Phase 4ï¼ˆãƒ‡ãƒ—ãƒ­ã‚¤ï¼‰ | ğŸŸ  ä½ | 1é€±é–“ | âš ï¸ æœ‰æ–™æ æ¨å¥¨ | â­â­ |

---

## è«–æ–‡å‚è€ƒãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### DDPM (Ho et al. 2020)
- [x] å‰å‘ãæ‹¡æ•£éç¨‹ã®ç†è§£
- [x] é€†å‘ãé™¤å»éç¨‹ã®ç†è§£
- [ ] ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°é–¢æ•°ã®æœ€é©åŒ–

### Latent Diffusion (Rombach et al. 2022)
- [x] VAE ã«ã‚ˆã‚‹æ½œåœ¨ç©ºé–“åœ§ç¸®ã®æ´»ç”¨
- [x] ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ (CLIP) ã®çµ±åˆ
- [ ] ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã®å¾®èª¿æ•´

### Text-to-Image Robustness (Gao et al. 2306.13103)
- [x] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‘‚å‹•ã¸ã®è€æ€§
- [x] è¤‡æ•°ã‚¿ã‚°ã«ã‚ˆã‚‹å …ç‰¢æ€§å¼·åŒ–
- [ ] æ•µå¯¾çš„æ”»æ’ƒã¸ã®é˜²å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

### LCM (Luo et al. 2023)
- [x] è’¸ç•™ã«ã‚ˆã‚‹æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—å‰Šæ¸›
- [x] ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼çµ±åˆ
- [ ] å“è³ª-é€Ÿåº¦ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã®æœ€é©åŒ–

---

## é–¢é€£ãƒ–ãƒ­ã‚°è¨˜äº‹è¨ˆç”»

1. **æ—¢å­˜è¨˜äº‹**
   - Day 1-2: Stable Diffusion åŸºç¤ âœ…
   - Day 3: PyTorch + Diffusers å®Ÿè£… âœ…
   - Day 4: GitHub å…¬é–‹ âœ…

2. **æ–°è¦è¨˜äº‹ï¼ˆPhase åˆ¥ï¼‰**
   - ğŸ“ ã€ŒDDPM ã‹ã‚‰ Latent Diffusion ã¸ - æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ã€
   - ğŸ“ ã€ŒLLMãƒ¡ãƒ‡ã‚£ã‚¢ã®ã‚ˆã†ã«å¤šæ®µéšãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã€
   - ğŸ“ ã€ŒPyTorchã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã€
   - ğŸ“ ã€ŒColab T4 ã§ 50 ã‚¨ãƒãƒƒã‚¯ LoRA å­¦ç¿’ã‚’å®Œé‚ã™ã‚‹ã€
   - ğŸ“ ã€Œ4ã‚¹ãƒ†ãƒƒãƒ— LCM ã«ã‚ˆã‚‹æ¨è«–é«˜é€ŸåŒ–ã®å®Ÿè£…ã€
   - ğŸ“ ã€Œæ½œåœ¨ç©ºé–“ã‚¢ãƒ¼ãƒˆã®åˆ¶ä½œ - Image-to-Image ã¸ã®é“ã€

---

## å‚è€ƒè«–æ–‡ãƒ»ãƒªã‚½ãƒ¼ã‚¹

1. **Ho, J., Jain, A., & Abbeel, P. (2020)**
   - Denoising Diffusion Probabilistic Models (DDPM)
   - https://arxiv.org/abs/2006.11239

2. **Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022)**
   - High-Resolution Image Synthesis with Latent Diffusion Models
   - https://arxiv.org/abs/2112.10752

3. **Gao, H., Zhang, H., Dong, Y., & Deng, Z. (2023)**
   - Evaluating the Robustness of Text-to-image Diffusion Models against Real-world Attacks
   - https://arxiv.org/abs/2306.13103

4. **Luo, S., Tan, Y., Huang, L., Li, J., & Zhao, H. (2023)**
   - LCM: Latent Consistency Models for Fast Image Generation
   - https://arxiv.org/abs/2310.04378

5. **ãƒªã‚½ãƒ¼ã‚¹**
   - [Hugging Face Diffusers](https://github.com/huggingface/diffusers)
   - [PEFT - Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft)
   - [xFormers - Memory-Efficient Attention](https://github.com/facebookresearch/xformers)
   - [ControlNet Implementation](https://github.com/lllyasviel/ControlNet)

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

### å³åº§ã®å®Ÿè£…ï¼ˆ1-2é€±é–“ï¼‰

1. **Phase 1 å®Ÿè£…é–‹å§‹**
   - ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã®æ§‹ç¯‰
   - Claude API ã¨ã®é€£æº
   - A/B ãƒ†ã‚¹ãƒˆã®æº–å‚™

2. **Phase 2A æ¤œè¨**
   - Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä½œæˆ
   - ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ†ã‚¹ãƒˆ
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¸¬å®š

3. **ãƒ–ãƒ­ã‚°è¨˜äº‹åŸ·ç­†é–‹å§‹**
   - è«–æ–‡ã‚µãƒãƒªãƒ¼è¨˜äº‹ã®ä½œæˆ
   - å®Ÿè£…è§£èª¬è¨˜äº‹ã®æº–å‚™

### ä¸­æœŸç›®æ¨™ï¼ˆ1ãƒ¶æœˆï¼‰

- [ ] Phase 1 å®Œå…¨å®Ÿè£… + ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] Phase 2A (LoRAå­¦ç¿’) å®Œæˆ
- [ ] 5 ã¤ã®ãƒ–ãƒ­ã‚°è¨˜äº‹å…¬é–‹
- [ ] GitHub Star 100+ ã‚’ç›®æŒ‡ã™

### é•·æœŸç›®æ¨™ï¼ˆ3ãƒ¶æœˆï¼‰

- [ ] Phase 2B (LCMè’¸ç•™) å®Œæˆ
- [ ] Phase 3 (ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«) å®Ÿè£…
- [ ] Streamlit ã‚¢ãƒ—ãƒªå…¬é–‹
- [ ] ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ»ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã§å¤§ããªã‚¢ãƒ”ãƒ¼ãƒ«

---

**æœ€çµ‚æ›´æ–°**: 2026å¹´2æœˆ19æ—¥

**è¨­è¨ˆåŸºæº–**: è«–æ–‡ãƒ™ãƒ¼ã‚¹å®Ÿè£…ã§å­¦è¡“çš„ä¿¡é ¼æ€§ã‚’ç¢ºä¿

**æ¬¡ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼**: Phase 1 å®Ÿè£…é–‹å§‹æ™‚
