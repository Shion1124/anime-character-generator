#!/usr/bin/env python3
"""
LoRA å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨ä¾‹:
    python validate_dataset.py
    python validate_dataset.py --data-dir training_data
"""

import os
from PIL import Image
from pathlib import Path
import argparse
import json


def validate_training_data(data_dir="training_data"):
    """å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯"""
    
    print("="*60)
    print("ğŸ“Š Training Dataset Validation")
    print("="*60)
    print(f"\nğŸ“ Checking: {data_dir}/\n")
    
    data_path = Path(data_dir)
    
    if not data_path.exists():
        print(f"âŒ Error: Directory not found: {data_dir}")
        return False
    
    total_images = 0
    total_size = 0
    issues = []
    style_stats = {}
    resolution_stats = {}
    
    for style_dir in sorted(data_path.iterdir()):
        if not style_dir.is_dir() or style_dir.name.startswith("."):
            continue
        
        if style_dir.name in ["images", "outputs"]:
            continue
        
        print(f"ğŸ“ {style_dir.name}/")
        style_count = 0
        style_size = 0
        min_res = (float('inf'), float('inf'))
        max_res = (0, 0)
        
        for img_file in sorted(style_dir.glob("*.png")):
            try:
                img = Image.open(img_file)
                width, height = img.size
                file_size = img_file.stat().st_size / (1024 * 1024)  # MB
                
                # è§£åƒåº¦ãƒã‚§ãƒƒã‚¯
                if width < 256 or height < 256:
                    issues.append(f"âš ï¸  Small image: {img_file.name} ({width}x{height})")
                
                if width > 2000 or height > 2000:
                    issues.append(f"âš ï¸  Large image: {img_file.name} ({width}x{height})")
                
                min_res = (min(min_res[0], width), min(min_res[1], height))
                max_res = (max(max_res[0], width), max(max_res[1], height))
                
                # å½¢å¼ãƒã‚§ãƒƒã‚¯
                if img.format != "PNG":
                    issues.append(f"âš ï¸  Not PNG: {img_file.name} ({img.format})")
                
                style_count += 1
                style_size += file_size
                total_images += 1
                total_size += file_size
                
            except Exception as e:
                issues.append(f"âŒ Corrupt: {img_file.name} - {str(e)[:50]}")
        
        if style_count > 0:
            style_stats[style_dir.name] = style_count
            resolution_stats[style_dir.name] = {
                "min": min_res,
                "max": max_res,
                "count": style_count,
                "size_mb": round(style_size, 2)
            }
            print(f"   âœ… {style_count} images ({round(style_size, 2)} MB)")
            print(f"      Resolution: {min_res} - {max_res}")
        else:
            print(f"   âš ï¸  No images found")
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    metadata_path = data_path / "metadata.json"
    if metadata_path.exists():
        try:
            with open(metadata_path, "r") as f:
                metadata = json.load(f)
            print(f"\nâœ… Metadata file exists (entries: {len(metadata.get('training_data', []))})")
        except Exception as e:
            print(f"âš ï¸  Metadata file error: {e}")
    else:
        print(f"âš ï¸  No metadata.json found")
    
    # ãƒ­ã‚°ç¢ºèª
    log_path = data_path / "download_log.txt"
    if log_path.exists():
        print(f"âœ… Log file exists")
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print(f"ğŸ“Š Summary:")
    print(f"   Total images: {total_images}")
    print(f"   Total size: {round(total_size, 2)} MB")
    
    print(f"\nğŸ¨ Styles:")
    for style, count in sorted(style_stats.items()):
        print(f"   {style}: {count} images")
    
    # å•é¡Œã®è¡¨ç¤º
    if issues:
        print(f"\nâš ï¸  Issues found: {len(issues)}")
        for issue in issues[:20]:  # æœ€åˆã®20ä»¶è¡¨ç¤º
            print(f"   {issue}")
    else:
        print(f"\nâœ… No issues found!")
    
    # æ¨å¥¨å€¤ãƒã‚§ãƒƒã‚¯
    print(f"\nğŸ¯ Recommendations:")
    if total_images >= 200:
        print(f"   âœ… Image count is sufficient for training")
    else:
        print(f"   âš ï¸  Minimum 200 images recommended (current: {total_images})")
    
    if total_size >= 1000:
        print(f"   âœ… Dataset size is good")
    else:
        print(f"   âš ï¸  Consider more images (current: {round(total_size, 2)} MB)")
    
    # è§£åƒåº¦ãƒã‚§ãƒƒã‚¯
    avg_widths = [stat["max"][0] for stat in resolution_stats.values()]
    if avg_widths and sum(avg_widths) / len(avg_widths) >= 512:
        print(f"   âœ… Resolution is adequate for training")
    else:
        print(f"   âš ï¸  Consider images closer to 512x512")
    
    print("\n" + "="*60)
    
    is_valid = total_images >= 200 and len(issues) < 5
    return is_valid


def main():
    parser = argparse.ArgumentParser(description="Training dataset validator")
    parser.add_argument(
        "--data-dir",
        default="training_data",
        help="Training data directory (default: training_data)"
    )
    
    args = parser.parse_args()
    
    try:
        is_valid = validate_training_data(args.data_dir)
        print(f"\n{'âœ… Ready for training!' if is_valid else 'âŒ Dataset needs improvements'}")
        return 0 if is_valid else 1
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
